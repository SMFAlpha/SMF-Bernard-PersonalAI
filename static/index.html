<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bernard AI Assistant</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <script src="https://cdn.socket.io/4.5.4/socket.io.min.js"></script>
    <style>
        .chat-container {
            height: calc(100vh - 200px);
        }
        .message {
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr 1fr 2fr;
            grid-template-rows: repeat(5, 130px);
            gap: 20px;
            max-width: 1400px;
            margin: 0 auto;
        }
        .button {
            background-color: #1a5276;
            color: white;
            border-radius: 8px;
            padding: 15px 10px;
            text-align: center;
            font-weight: bold;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            transition: background-color 0.3s;
            border: none;
            font-size: 16px;
            height: 100%;
            width: 100%;
        }
        .button:hover {
            background-color: #154360;
        }
        .subject-button {
            background-color: #1a5276; /* Start as blue */
            color: white;
            border-radius: 8px;
            padding: 15px 10px;
            text-align: center;
            font-weight: bold;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            transition: background-color 0.3s;
            border: none;
            font-size: 16px;
            height: 100%;
            width: 100%;
        }.text-box {
            grid-column: 2 / span 3;
            border: 1px solid #999;
            padding: 15px;
            background-color: white;
            display: flex;
            flex-direction: column;
        }
        textarea {
            width: 100%;
            height: 80px;
            resize: none;
            margin-top: 5px;
            font-family: Arial, sans-serif;
            padding: 8px;
        }
        .response {
            grid-column: 5;
            background-color: #1a5276;
            color: white;
            padding: 20px;
            border-radius: 8px;
            font-weight: bold;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            position: relative;
            cursor: pointer;
        }
        .response textarea {
            width: 100%;
            height: 110px;
            background-color: rgba(255, 255, 255, 0.2);
            color: white;
            border: none;
            border-radius: 4px;
            padding: 8px;
            margin-bottom: 5px;
            cursor: pointer;
        }
        .response-label {
            margin-top: 5px;
            text-align: center;
            font-size: 12px;
        }
        .system-window {
            grid-column: 2 / span 3;
            border: 1px solid #999;
            padding: 15px;
            background-color: white;
            overflow-y: auto;
        }
        #systemMessages {
            height: 100px;
            overflow-y: auto;
        }
        /* Hide the audio player */
        #audioPlayer {
            display: none;
        }
        
        /* Modal styles */
        .modal {
            display: none;
            position: fixed;
            z-index: 1;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            overflow: auto;
            background-color: rgba(0,0,0,0.4);
        }
        
        .modal-content {
            background-color: #fefefe;
            margin: 10% auto;
            padding: 20px;
            border: 1px solid #888;
            width: 70%;
            border-radius: 10px;
            max-width: 800px;
        }
        
        .close-btn {
            color: #aaa;
            float: right;
            font-size: 28px;
            font-weight: bold;
            cursor: pointer;
        }
        
        .close-btn:hover {
            color: black;
        }
        
        /* Subject management styles - preserved as original */
        .subject-container {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin-top: 15px;
            margin-bottom: 15px;
        }
        
        .subject-item {
            background-color: #1a5276;
            color: white;
            padding: 10px 15px;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s;
        }
        
        .subject-item:hover {
            background-color: #154360;
        }
        
        .new-subject {
            margin-top: 15px;
            border-top: 1px solid #ccc;
            padding-top: 15px;
        }
        
        .add-subject-btn {
            margin-top: 10px;
            padding: 8px 15px;
            background-color: #1a5276;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
        
        .add-subject-btn:hover {
            background-color: #154360;
        }/* Subject detail form styles */
        .subject-form {
            margin-top: 15px;
            display: none;
        }
        
        .subject-detail-area {
            width: 100%;
            min-height: 100px;
            margin-top: 10px;
            font-family: Arial, sans-serif;
            padding: 10px;
            resize: vertical;
        }
        
        .subject-input {
            width: 100%;
            padding: 8px;
            margin-bottom: 10px;
        }
        
        .button-row {
            display: flex;
            justify-content: space-between;
            gap: 10px;
            margin-top: 10px;
        }
        
        .button-row button {
            padding: 8px 15px;
            background-color: #1a5276;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
        
        .button-row button:hover {
            background-color: #154360;
        }
        
        .subject-section {
            margin-bottom: 20px;
        }
        
        .subject-title {
            font-weight: bold;
            margin-bottom: 5px;
        }

        /* Toggle switch styles */
        .toggles-container {
            display: flex;
            flex-wrap: wrap;
            justify-content: space-between;
            margin-top: 10px;
            grid-column: 2 / span 3;
            gap: 10px;
        }
        
        .toggle-group {
            display: flex;
            flex-direction: column;
            align-items: center;
            flex: 1;
            min-width: 150px;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 10px;
            background-color: #f9f9f9;
        }
        
        .toggle-label {
            font-weight: bold;
            margin-bottom: 5px;
        }
        
        .toggle-description {
            font-size: 12px;
            text-align: center;
            margin-bottom: 10px;
            color: #666;
            height: 40px;
        }
        
        .switch {
            position: relative;
            display: inline-block;
            width: 60px;
            height: 34px;
        }
        
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 34px;
        }
        
        .slider:before {
            position: absolute;
            content: "";
            height: 26px;
            width: 26px;
            left: 4px;
            bottom: 4px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        
        input:checked + .slider {
            background-color: #1a5276;
        }
        
        input:focus + .slider {
            box-shadow: 0 0 1px #1a5276;
        }
        
        input:checked + .slider:before {
            transform: translateX(26px);
        }
        
        /* New styles for conversation phrases */
        .phrase-container {
            display: flex;
            flex-direction: column;
            gap: 10px;
            margin-top: 15px;
            margin-bottom: 15px;
        }
        
        .phrase-item {
            display: block;
            width: 100%;
            text-align: left;
            margin-bottom: 10px;
            padding: 15px;
            background-color: #1a5276;
            color: white;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s;
        }
        
        .phrase-item:hover {
            background-color: #154360;
        }
        
        .new-phrase {
            margin-top: 15px;
            border-top: 1px solid #ccc;
            padding-top: 15px;
        }
        
        .add-phrase-btn, .repeat-btn {
            margin-top: 10px;
            padding: 8px 15px;
            background-color: #1a5276;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
        
        .add-phrase-btn:hover, .repeat-btn:hover {
            background-color: #154360;
        }
        
        .phrase-form {
            margin-top: 15px;
            display: none;
        }
        
        .phrase-detail-area {
            width: 100%;
            min-height: 100px;
            margin-top: 10px;
            font-family: Arial, sans-serif;
            padding: 10px;
            resize: vertical;
        }
        
        .phrase-section {
            margin-bottom: 20px;
        }
        
        .phrase-title {
            font-weight: bold;
            margin-bottom: 5px;
        }
        
        .repeat-btn-container {
            display: flex;
            justify-content: center;
            margin-top: 15px;
            border-top: 1px solid #ccc;
            padding-top: 15px;
        }
        
        /* Add CSS for the status indicator */
        .status-indicator {
            background-color: #f8d7da;
            border: 1px solid #f5c6cb;
            color: #721c24;
            padding: 10px;
            border-radius: 5px;
            margin: 10px 0;
            text-align: center;
            grid-column: 2 / span 3;
        }
        /* Add these style rules to the existing CSS */

        /* Split button container styling */
        .button-split-container {
            display: flex;
            width: 100%;
            height: 100%;
            grid-column: span 1;
            gap: 5px;
        }

        /* Split button styling */
        .split-button {
            background-color: #1a5276;
            color: white;
            border-radius: 8px;
            padding: 15px 10px;
            text-align: center;
            font-weight: bold;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            transition: background-color 0.3s;
            border: none;
            font-size: 16px;
            flex: 1;
            width: 50%;
            height: 100%;
        }

        .split-button:hover {
            background-color: #154360;
        }
        /* Settings Modal Styles */
        .settings-tabs {
            display: flex;
            border-bottom: 1px solid #ccc;
            margin-bottom: 20px;
        }

        .settings-tab-btn {
            background-color: #f1f1f1;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 12px 16px;
            transition: 0.3s;
            font-size: 16px;
            border-radius: 5px 5px 0 0;
            margin-right: 4px;
        }

        .settings-tab-btn:hover {
            background-color: #ddd;
        }

        .settings-tab-btn.active {
            background-color: #1a5276;
            color: white;
        }

        .settings-tab-content {
            display: none;
            padding: 15px;
            border-top: none;
        }

        .settings-form {
            display: flex;
            flex-direction: column;
            gap: 15px;
        }

        .settings-field {
            display: flex;
            flex-direction: column;
            gap: 5px;
        }

        .settings-input, .settings-textarea, .settings-range {
            padding: 8px;
            border: 1px solid #ccc;
            border-radius: 4px;
            font-size: 14px;
        }

        .settings-textarea {
            font-family: monospace;
            resize: vertical;
        }

        .settings-range {
            width: 100%;
        }

        .settings-checkbox {
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .settings-help {
            margin: 0;
            font-size: 12px;
            color: #666;
        }

        .settings-actions {
            display: flex;
            justify-content: flex-end;
            gap: 10px;
            margin-top: 20px;
        }

        .settings-btn {
            padding: 10px 15px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-weight: bold;
        }

        .settings-test-btn {
            background-color: #3498db;
            color: white;
        }

        .settings-save-btn {
            background-color: #2ecc71;
            color: white;
        }

        .settings-reset-btn {
            background-color: #e74c3c;
            color: white;
        }

        h4 {
            margin: 15px 0 5px 0;
            border-bottom: 1px solid #eee;
            padding-bottom: 5px;
        }
        .model-settings {
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #eee;
        }
        .language-buttons {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 10px;
            margin-top: 20px;
        }

        .language-buttons button {
            padding: 15px;
            background-color: #1a5276;
            color: white;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            transition: background-color 0.3s;
        }

        .language-buttons button:hover {
            background-color: #154360;
        }

        .file-upload-section {
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid #ccc;
        }

        .file-list-section {
            margin-top: 20px;
        }

        .file-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            background-color: #f5f5f5;
            padding: 10px;
            margin: 5px 0;
            border-radius: 5px;
        }

        .file-item-info {
            flex-grow: 1;
        }

        .file-item-actions {
            display: flex;
            gap: 10px;
        }

        .delete-btn {
            background-color: #dc3545;
            color: white;
            border: none;
            padding: 5px 10px;
            border-radius: 3px;
            cursor: pointer;
        }

        .delete-btn:hover {
            background-color: #c82333;
        }

        .file-status {
            font-size: 12px;
            color: #666;
        }

        #fileInput {
            margin-bottom: 10px;
            padding: 8px;
        }
    </style>
</head>
<body>
    <div class="container">
            <!-- Row 1 -->
        <button class="button" onclick="retrieveContext()">RETRIEVE CONTEXT</button>
        
        <!-- Split "Conversation Starters" into two equal-width buttons -->
        <div class="button-split-container">
            <button class="split-button" onclick="showStarterModal()">CONVERSATION STARTERS</button>
            <button class="split-button" onclick="showEnderModal()">CONVERSATION ENDERS</button>
        </div>
        
        <button id="subjectButton" class="subject-button" onclick="showSubjectModal()">SELECT SUBJECT</button>
        
        <!-- Split "Start Recording" into two equal-width buttons -->
        <div class="button-split-container">
            <button class="split-button" onclick="startRecording()">START AUTO RECORDING</button>
            <button class="split-button" id="stopRecordingBtn" onclick="toggleManualRecording()">START MANUAL RECORDING</button>
        </div>
        
        <div class="response" onclick="selectReply('return1')">
            <textarea id="return1" readonly></textarea>
        </div>
        
        <!-- Row 2 remains unchanged -->
        <button class="button" onclick="uploadDoc()">UPLOAD DOCUMENT TO KNOWLEDGE BASE</button>
        <div class="text-box">
            <textarea id="textInput" placeholder="Text box for input/output. Will be filled with transcripts automatically."></textarea>
        </div>
        <div class="response" onclick="selectReply('return2')">
            <textarea id="return2" readonly></textarea>
        </div>
        
        <!-- Row 3 changes -->
        <button class="button" onclick="showLogMemoryModal()">LOG MEMORY</button>
        
        <!-- Replace "END CONVERSATION" with two equal-width buttons -->
        <div class="button-split-container">
            <button class="split-button" onclick="translateToDutch()">Translate to Dutch</button>
            <button class="split-button" onclick="showLanguageSelector()">Translate from Dutch</button>
        </div>
        
        <button class="button" onclick="startConversationWithRecording()">SPEAK</button>
        
        <!-- Split "SEND TO LLM" into two equal-width buttons -->
        <div class="button-split-container">
            <button class="split-button" onclick="sendLLM()">(RE)SEND TO LLM</button>
            <button class="split-button" onclick="enhancePrompt()">ENHANCE PROMPT</button>
        </div>
        
        <div class="response" onclick="selectReply('return3')">
            <textarea id="return3" readonly></textarea>
        </div>
        <!-- Row 4 -->
        <button class="button" onclick="updateVector()">UPDATE VECTOR STORE</button>
        <div class="system-window">
            <div id="systemMessages">Window For system messages.</div>
        </div>
        <div class="response" onclick="selectReply('return4')">
            <textarea id="return4" readonly></textarea>
        </div>
        
        <!-- Row 5 -->
        <button class="button" onclick="showSettings()">SETTINGS</button>
        
        <!-- Feature Toggles -->
        <div class="toggles-container">
            <div class="toggle-group">
                <div class="toggle-label">Context</div>
                <div class="toggle-description">Add additional context to the prompt sent to OpenAI</div>
                <label class="switch">
                    <input type="checkbox" id="contextToggle" checked>
                    <span class="slider"></span>
                </label>
            </div>
            <div class="toggle-group">
                <div class="toggle-label">Auto Mode</div>
                <div class="toggle-description">Auto-answer with a single response</div>
                <label class="switch">
                    <input type="checkbox" id="autoModeToggle">
                    <span class="slider"></span>
                </label>
            </div>
            <div class="toggle-group">
                <div class="toggle-label">Internet</div>
                <div class="toggle-description">Add internet-based context to the chat completion query</div>
                <label class="switch">
                    <input type="checkbox" id="internetToggle">
                    <span class="slider"></span>
                </label>
            </div>
        </div>
        <!-- Add this after the toggles-container div, around line 674 -->
        <div id="statusIndicator" class="status-indicator" style="display: none;"></div>
    </div>
    
    <!-- Audio player (hidden) -->
    <audio id="audioPlayer"></audio>
    
    <!-- Subject selection modal -->
    <div id="subjectModal" class="modal">
        <div class="modal-content">
            <span class="close-btn" onclick="closeSubjectModal()">&times;</span>
            <h2>Subject Management</h2>
            
            <!-- Subject list section -->
            <div class="subject-section">
                <div class="subject-title">Select a Subject:</div>
                <div id="subjectContainer" class="subject-container">
                    <!-- Subject items will be loaded here -->
                </div>
            </div>
            
            <!-- Subject form - initially hidden until a subject is selected or Add New is clicked -->
            <div id="subjectForm" class="subject-form">
                <h3 id="subjectFormTitle">Subject Details</h3>
                <input type="text" id="subjectNameInput" class="subject-input" placeholder="Subject name (e.g. person's name)">
                <textarea id="subjectDescriptionInput" class="subject-detail-area" placeholder="Enter details about this subject that would help Bernard contextualize the conversation (e.g., 'John is a 45-year-old researcher who has been working on ALS treatments for 10 years. We've met twice at conferences in Amsterdam.')"></textarea>
                <div class="button-row">
                    <button id="cancelSubjectBtn" onclick="cancelSubjectEdit()">Cancel</button>
                    <button id="saveSubjectBtn" onclick="saveSubjectDetails()">Save Subject</button>
                </div>
            </div>
            
            <!-- Add New button at the bottom -->
            <div class="new-subject">
                <button class="add-subject-btn" onclick="showNewSubjectForm()">Add New Subject</button>
            </div>
        </div>
    </div>
    
    <!-- Conversation Starters modal -->
    <div id="starterModal" class="modal">
        <div class="modal-content">
            <span class="close-btn" onclick="closeStarterModal()">&times;</span>
            <h2>Conversation Starters</h2>
            
            <!-- Starters list section -->
            <div class="phrase-section">
                <div class="phrase-title">Select a Conversation Starter:</div>
                <div id="starterContainer" class="phrase-container">
                    <!-- Starter items will be loaded here -->
                </div>
            </div>
            
            <!-- Starter form - initially hidden until a phrase is selected or Add New is clicked -->
            <div id="starterForm" class="phrase-form">
                <h3 id="starterFormTitle">Conversation Starter Details</h3>
                <textarea id="starterTextInput" class="phrase-detail-area" placeholder="Enter a conversation starter phrase that Bernard will speak to begin a conversation."></textarea>
                <div class="button-row">
                    <button id="cancelStarterBtn" onclick="cancelStarterEdit()">Cancel</button>
                    <button id="saveStarterBtn" onclick="saveStarterDetails()">Save Phrase</button>
                </div>
            </div>
            
            <!-- Repeat button at the bottom -->
            <div class="repeat-btn-container">
                <button class="repeat-btn" onclick="sayAgainFromModal()">Say That Again</button>
            </div>
            
            <!-- Add New button at the bottom -->
            <div class="new-phrase">
                <button class="add-phrase-btn" onclick="showNewStarterForm()">Add New Starter</button>
            </div>
        </div>
    </div>
    
    <!-- Conversation Enders modal -->
    <div id="enderModal" class="modal">
        <div class="modal-content">
            <span class="close-btn" onclick="closeEnderModal()">&times;</span>
            <h2>Conversation Enders</h2>
            
            <!-- Enders list section -->
            <div class="phrase-section">
                <div class="phrase-title">Select a Conversation Ender:</div>
                <div id="enderContainer" class="phrase-container">
                    <!-- Ender items will be loaded here -->
                </div>
            </div>
            
            <!-- Ender form - initially hidden until a phrase is selected or Add New is clicked -->
            <div id="enderForm" class="phrase-form">
                <h3 id="enderFormTitle">Conversation Ender Details</h3>
                <textarea id="enderTextInput" class="phrase-detail-area" placeholder="Enter a conversation ending phrase that Bernard will speak to conclude a conversation."></textarea>
                <div class="button-row">
                    <button id="cancelEnderBtn" onclick="cancelEnderEdit()">Cancel</button>
                    <button id="saveEnderBtn" onclick="saveEnderDetails()">Save Phrase</button>
                </div>
            </div>
            
            <!-- Add New button at the bottom -->
            <div class="new-phrase">
                <button class="add-phrase-btn" onclick="showNewEnderForm()">Add New Ender</button>
            </div>
        </div>
    </div>
    <!-- Log Memory modal -->
    <div id="logMemoryModal" class="modal">
        <div class="modal-content">
            <span class="close-btn" onclick="closeLogMemoryModal()">&times;</span>
            <h2>Log Memory</h2>
            
            <div class="settings-form">
                <div class="settings-field">
                    <label for="memoryTitle">Title:</label>
                    <input type="text" id="memoryTitle" name="memoryTitle" autocomplete="off">
                </div>
                
                <div class="settings-field">
                    <label for="memoryTags">Tags (comma separated):</label>
                    <input type="text" id="memoryTags" name="memoryTags" autocomplete="off">
                </div>
                
                <div class="settings-field">
                    <label for="memoryDate">Date:</label>
                    <input type="date" id="memoryDate" name="memoryDate" autocomplete="off">
                </div>
                
                <div class="settings-field">
                    <label for="memoryText">Memory Text:</label>
                    <textarea id="memoryText" name="memoryText" autocomplete="off" rows="8" placeholder="Enter details about this memory..."></textarea>
                </div>
                
                <div class="button-row">
                    <button id="cancelMemoryBtn" onclick="closeLogMemoryModal()">Cancel</button>
                    <button id="saveMemoryBtn" onclick="saveMemoryLog()">Save Memory</button>
                </div>
            </div>
        </div>
    </div>
    <!-- Settings Modal -->
    <div id="settingsModal" class="modal">
        <div class="modal-content" style="width: 80%; max-width: 1000px;">
            <span class="close-btn" onclick="closeSettingsModal()">&times;</span>
            <h2>Bernard Settings</h2>
            
            <!-- Tab Navigation -->
            <div class="settings-tabs">
                <button class="settings-tab-btn active" onclick="openSettingsTab(event, 'voiceSettings')">Voice Settings</button>
                <button class="settings-tab-btn" onclick="openSettingsTab(event, 'llmSettings')">LLM Settings</button>
                <button class="settings-tab-btn" onclick="openSettingsTab(event, 'recorderSettings')">Recorder Settings</button>
                <button class="settings-tab-btn" onclick="openSettingsTab(event, 'systemSettings')">System Settings</button>
            </div>
            
            <!-- Voice Settings Tab -->
            <div id="voiceSettings" class="settings-tab-content" style="display: block;">
                <h3>Voice Settings (ElevenLabs)</h3>
                <div class="settings-form">
                    <div class="settings-field">
                        <label for="elevenLabsApiKey">API Key:</label>
                        <input type="password" id="elevenLabsApiKey" name="elevenLabsApiKey" autocomplete="off">
                    </div>
                    <div class="settings-field">
                        <label for="voiceId">Voice ID:</label>
                        <input type="text" id="voiceId" name="voiceId" autocomplete="off">
                    </div>
                    <div class="settings-field">
                        <label for="ttsModel">TTS Model:</label>
                        <select id="ttsModel" name="ttsModel">
                            <option value="eleven_turbo_v2_5">eleven_turbo_v2_5 (Lowest Latency)</option>
                            <option value="eleven_flash_v2_5">eleven_flash_v2_5 (Low Latency)</option>
                            <option value="eleven_multilingual_v2">eleven_multilingual_v2</option>
                            <option value="eleven_multilingual_v1">eleven_multilingual_v1</option>
                            <option value="eleven_monolingual_v1">eleven_monolingual_v1</option>
                        </select>
                    </div>
                    <div class="settings-field">
                        <label for="outputFormat">Output Format:</label>
                        <select id="outputFormat" name="outputFormat">
                            <option value="mp3_44100_192">mp3_44100_192</option>
                            <option value="mp3_44100_128">mp3_44100_128</option>
                            <option value="mp3_44100_64">mp3_44100_64</option>
                            <option value="wav">WAV</option>
                        </select>
                    </div>
                    <div class="settings-field">
                        <label for="voiceSeed">Seed (for consistency):</label>
                        <input type="number" id="voiceSeed" name="voiceSeed" min="0" value="8675309">
                    </div>
                    
                    <h4>Voice Parameters</h4>
                    <div class="settings-field">
                        <label for="voiceStability">Stability (0.0-1.0):</label>
                        <input type="range" id="voiceStability" name="voiceStability" min="0" max="1" step="0.05" value="0.5">
                        <span id="voiceStabilityValue">0.5</span>
                    </div>
                    <div class="settings-field">
                        <label for="voiceSimilarity">Similarity Boost (0.0-1.0):</label>
                        <input type="range" id="voiceSimilarity" name="voiceSimilarity" min="0" max="1" step="0.05" value="0.75">
                        <span id="voiceSimilarityValue">0.75</span>
                    </div>
                    <div class="settings-field">
                        <label for="voiceSpeed">Speed (0.5-2.0):</label>
                        <input type="range" id="voiceSpeed" name="voiceSpeed" min="0.5" max="2.0" step="0.05" value="0.9">
                        <span id="voiceSpeedValue">0.9</span>
                    </div>
                    <div class="settings-field">
                        <label for="voiceStyle">Style (0.0-1.0):</label>
                        <input type="range" id="voiceStyle" name="voiceStyle" min="0" max="1" step="0.05" value="0.3">
                        <span id="voiceStyleValue">0.3</span>
                    </div>

                    <div class="settings-actions">
                        <button class="settings-btn settings-test-btn" onclick="testVoiceSettings()">Test Voice</button>
                        <button class="settings-btn settings-save-btn" onclick="saveVoiceSettings()">Save Settings</button>
                    </div>
                </div>
            </div>
            
            <!-- LLM Settings Tab -->
            <div id="llmSettings" class="settings-tab-content">
                <h3>LLM Settings</h3>
                <div class="settings-form">
                    <!-- Provider selection dropdown -->
                    <div class="settings-field">
                        <label for="llmProvider">LLM Provider:</label>
                        <select id="llmProvider" name="llmProvider">
                            <option value="openai">OpenAI</option>
                            <option value="groq">Groq</option>
                        </select>
                    </div>
                    
                    <!-- OpenAI models section - shown when OpenAI is selected -->
                    <div id="openaiModels" class="model-settings">
                        <div class="settings-field">
                            <label for="openaiApiKey">OpenAI API Key:</label>
                            <input type="password" id="openaiApiKey" name="openaiApiKey" autocomplete="off">
                        </div>
                        <div class="settings-field">
                            <label for="openaiModel">Model:</label>
                            <select id="openaiModel" name="openaiModel">
                                <option value="gpt-4.1-mini">GPT-4.1-mini</option>
                                <option value="ft:gpt-4.1-mini-2025-04-14:personal::BS7YzK6D">Fine-tuned GPT-4.1-mini</option>
                                <option value="gpt-4-turbo">GPT-4 Turbo</option>
                                <option value="gpt-3.5-turbo">GPT-3.5 Turbo</option>
                                <option value="ft:gpt-3.5-turbo-0125:personal::BJicSA0X">Bernard Fine-tuned 3.5 Turbo</option>
                                <option value="ft:gpt-4o-2024-08-06:personal::BJjqCVq3">Bernard Fine-tuned GPT-4o</option>
                            </select>
                        </div>
                    </div>
                    
                    <!-- Groq models section - shown when Groq is selected -->
                    <div id="groqModels" class="model-settings" style="display: none;">
                        <div class="settings-field">
                            <label for="groqApiKey">Groq API Key:</label>
                            <input type="password" id="groqApiKey" name="groqApiKey" autocomplete="off">
                        </div>
                        <div class="settings-field">
                            <label for="groqModel">Model:</label>
                            <select id="groqModel" name="groqModel">
                                <option value="llama-3.3-70b-specdec">Llama-3.3-70B-SpecDec</option>
                                <option value="llama-3.1-70b-instant">Llama-3.1-70B-Instant</option>
                                <option value="mixtral-8x7b-32768">Mixtral-8x7B-32768</option>
                            </select>
                        </div>
                    </div>
                    
                    <!-- Common settings for all providers -->
                    <div class="settings-field">
                        <label for="llmTemperature">Temperature (0.0-1.0):</label>
                        <input type="range" id="llmTemperature" name="llmTemperature" min="0" max="1" step="0.05" value="0.9">
                        <span id="llmTemperatureValue">0.9</span>
                    </div>
                    <div class="settings-field">
                        <label for="maxTokens">Max Tokens:</label>
                        <input type="number" id="maxTokens" name="maxTokens" min="50" max="500" value="150">
                    </div>
                    <div class="settings-field">
                        <label for="completionsCount">Number of Completions (1-4):</label>
                        <input type="number" id="completionsCount" name="completionsCount" min="1" max="4" value="4">
                    </div>
                    
                    <h4>Perplexity (Internet Search)</h4>
                    <div class="settings-field">
                        <label for="perplexityApiKey">Perplexity API Key:</label>
                        <input type="password" id="perplexityApiKey" name="perplexityApiKey" autocomplete="off">
                    </div>
                    <div class="settings-field">
                        <label for="perplexityModel">Model:</label>
                        <select id="perplexityModel" name="perplexityModel">
                            <option value="sonar">Sonar</option>
                            <option value="sonar-small">Sonar Small</option>
                            <option value="sonar-medium">Sonar Medium</option>
                        </select>
                    </div>
                    
                    <div class="settings-actions">
                        <button class="settings-btn settings-test-btn" onclick="testLLMSettings()">Test LLM</button>
                        <button class="settings-btn settings-save-btn" onclick="saveLLMSettings()">Save Settings</button>
                    </div>
                </div>
            </div>
            
            <!-- Recorder Settings Tab -->
            <div id="recorderSettings" class="settings-tab-content">
                <h3>Audio Recorder Settings</h3>
                <div class="settings-form">
                    <div class="settings-field">
                        <label for="silenceThreshold">Silence Threshold (1-2000):</label>
                        <input type="number" id="silenceThreshold" name="silenceThreshold" min="0" max="2000" value="500">
                        <p class="settings-help">Higher values mean more noise will be considered as silence. Default: 500</p>
                    </div>
                    <div class="settings-field">
                        <label for="silenceDuration">Silence Duration (seconds):</label>
                        <input type="number" id="silenceDuration" name="silenceDuration" min="1" max="10" step="0.5" value="5">
                        <p class="settings-help">How many seconds of silence to wait before auto-stopping. Default: 5.0</p>
                    </div>
                    <div class="settings-field">
                        <label for="minRecordingDuration">Minimum Recording Duration (seconds):</label>
                        <input type="number" id="minRecordingDuration" name="minRecordingDuration" min="0.5" max="5" step="0.5" value="1">
                        <p class="settings-help">Minimum recording time before silence detection is enabled. Default: 1.0</p>
                    </div>
                    <div class="settings-field">
                        <label for="maxRecordingDuration">Maximum Recording Duration (seconds):</label>
                        <input type="number" id="maxRecordingDuration" name="maxRecordingDuration" min="10" max="300" value="60">
                        <p class="settings-help">Maximum recording time before auto-stopping. Default: 60.0</p>
                    </div>
                    
                    <div class="settings-field">
                        <label>Audio Processing:</label>
                        <div class="settings-checkbox">
                            <input type="checkbox" id="useNoiseReduction" name="useNoiseReduction" checked>
                            <label for="useNoiseReduction">Enable Noise Reduction</label>
                        </div>
                    </div>
                    
                    <div class="settings-actions">
                        <button class="settings-btn settings-test-btn" onclick="testRecorderSettings()">Test Recording</button>
                        <button class="settings-btn settings-save-btn" onclick="saveRecorderSettings()">Save Settings</button>
                    </div>
                </div>
            </div>
            
            <!-- System Settings Tab -->
            <div id="systemSettings" class="settings-tab-content">
                <h3>System Settings</h3>
                <div class="settings-form">
                    <div class="settings-field">
                        <label for="systemPrompt">System Prompt:</label>
                        <textarea id="systemPrompt" name="systemPrompt" rows="10"></textarea>
                    </div>
                    
                    <h4>File Locations</h4>
                    <div class="settings-field">
                        <label for="kbDir">Knowledge Base Directory:</label>
                        <input type="text" id="kbDir" name="kbDir" value="data/kb">
                    </div>
                    <div class="settings-field">
                        <label for="vectorStoreDir">Vector Store Directory:</label>
                        <input type="text" id="vectorStoreDir" name="vectorStoreDir" value="data/vector_store">
                    </div>
                    <div class="settings-field">
                        <label for="chatHistoryFile">Chat History File:</label>
                        <input type="text" id="chatHistoryFile" name="chatHistoryFile" value="data/chat_history.json">
                    </div>
                    <div class="settings-field">
                        <label for="phrasesFile">Phrases File:</label>
                        <input type="text" id="phrasesFile" name="phrasesFile" value="data/phrases.json">
                    </div>
                    
                    <h4>Language Settings</h4>
                    <div class="settings-field">
                        <label for="defaultLanguage">Default Language:</label>
                        <select id="defaultLanguage" name="defaultLanguage">
                            <option value="auto">Auto Detect</option>
                            <option value="nl">Dutch</option>
                            <option value="en">English</option>
                        </select>
                    </div>
                    <div class="settings-field">
                        <label>Language Detection:</label>
                        <div class="settings-checkbox">
                            <input type="checkbox" id="useLanguageDetection" name="useLanguageDetection" checked>
                            <label for="useLanguageDetection">Enable Language Detection</label>
                        </div>
                    </div>
                    
                    <h4>Advanced</h4>
                    <div class="settings-field">
                        <label>Debug Mode:</label>
                        <div class="settings-checkbox">
                            <input type="checkbox" id="debugMode" name="debugMode">
                            <label for="debugMode">Enable Debug Logging</label>
                        </div>
                    </div>
                    
                    <div class="settings-actions">
                        <button class="settings-btn settings-reset-btn" onclick="resetSystemSettings()">Reset to Defaults</button>
                        <button class="settings-btn settings-save-btn" onclick="saveSystemSettings()">Save Settings</button>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- Add language selector modal -->
    <div id="languageSelectorModal" class="modal">
        <div class="modal-content">
            <span class="close-btn" onclick="closeLanguageSelector()">&times;</span>
            <h2>Select Target Language</h2>
            <div class="language-buttons">
                <button onclick="translateFromDutch('en')">English</button>
                <button onclick="translateFromDutch('cs')">Czech</button>
                <button onclick="translateFromDutch('sk')">Slovak</button>
                <button onclick="translateFromDutch('pl')">Polish</button>
                <button onclick="translateFromDutch('de')">German</button>
                <button onclick="translateFromDutch('fr')">French</button>
                <button onclick="translateFromDutch('es')">Spanish</button>
                <button onclick="translateFromDutch('it')">Italian</button>
            </div>
        </div>
    </div>
    <!-- File Management Modal -->
    <div id="fileManagementModal" class="modal">
        <div class="modal-content">
            <span class="close-btn" onclick="closeFileManagementModal()">&times;</span>
            <h2>Knowledge Base File Management</h2>
            
            <!-- File Upload Section -->
            <div class="file-upload-section">
                <h3>Upload New File</h3>
                <input type="file" id="fileInput" accept=".pdf,.txt,.json,.doc,.docx,.md,.py,.js,.java,.cs,.cpp,.c,.go,.rb,.php,.html,.css,.sh">
                <button class="add-subject-btn" onclick="uploadFile()">Upload File</button>
            </div>
            
            <!-- File List Section -->
            <div class="file-list-section" style="margin-top: 20px;">
                <h3>Current Files in Knowledge Base</h3>
                <div id="fileListContainer">
                    <!-- Files will be loaded here dynamically -->
                </div>
            </div>
        </div>
    </div>
    <script>
        // Variables to store the current subject and phrases being edited
        let currentEditingSubject = null;
        let currentEditingStarter = null;
        let currentEditingEnder = null;
        let isCreatingNewSubject = false;
        let isCreatingNewStarter = false;
        let isCreatingNewEnder = false;
        let lastBernardStatement = "";
        let recordingCheckInterval = null;
        // Settings Modal Functions
        let currentSettingsData = {};
        // Global variables for audio recording
        let mediaRecorder = null;
        let audioChunks = [];
        let isRecording = false;
        let silenceDetectionInterval = null;
        let audioContext = null;
        let analyzer = null;
        let lastAudioLevel = 0;
        let lastAudioTime = 0;
        let silenceThreshold = 0.50; // Adjust based on testing
        let silenceDuration = 3000; // 5 seconds of silence to auto-stop
        let recordingStartTime = 0;
        let rawAudioChunks = []; // To store raw audio data for WAV conversion
        let continuousRecording = false;
        let voiceActivationEnabled = false;
        let silenceDetectionTimeout = null;
        let audioData = [];
        let isProcessingTranscript = false;
        let voiceActivationStream = null;
        let transcriptQueue = []; //buffering voice segments and transcripts
        let processingQueue = false;
        let activeTranscriptions = 0;
        const MAX_CONCURRENT_TRANSCRIPTIONS = 3;
        let currentConversation = {
            exchanges: [],
            lastUserInput: "",
            lastBernardResponse: "", 
            pendingResponses: []
        };
        // Initialize Socket.IO
        let socket = io();
        
        // Declare actualSampleRate as a global variable at the top of the script
        let actualSampleRate = 44100; // Default value
        
        // Handle connection events
        socket.on('connect', () => {
            console.log('Connected to server');
        });

        // Handle status updates
        socket.on('status_update', (data) => {
            console.log('Status update:', data.status);
            // Update UI with status - With safety check
            const statusIndicator = document.getElementById('statusIndicator');
            if (statusIndicator) {
                statusIndicator.textContent = data.status;
                statusIndicator.style.display = 'block';
            }
            
            // Also log to the system messages area as a fallback
            logMessage(data.status);
        });

        // Handle transcript updates
        socket.on('status_update', (data) => {
            console.log('Status update:', data.status);
            // Update UI with status - With safety check
            const statusIndicator = document.getElementById('statusIndicator');
            if (statusIndicator) {
                statusIndicator.textContent = data.status;
                statusIndicator.style.display = 'block';
            }
            
            // Also log to the system messages area as a fallback
            logMessage(data.status);
        });

        // Handle LLM chunks
        socket.on('llm_chunk', (data) => {
            console.log('LLM chunk received');
            // Update UI with the latest LLM response
            updateResponseDisplay(data.full_response_so_far);
        });

        // Handle TTS chunks
        socket.on('tts_chunk_ready', (data) => {
            console.log('TTS chunk ready:', data);
            // Play the audio chunk
            playAudioChunk(data.audio_url, data.chunk_index, data.is_last);
        });

        // Handle pipeline completion
        socket.on('pipeline_complete', (data) => {
            console.log('Pipeline complete:', data);
            
            // Store Bernard's statement for future context
            if (data.response) {
                lastBernardStatement = data.response;
            }
            
            // Update UI to show completion
            document.getElementById('statusIndicator').textContent = "Processing complete. Ready for next input.";
            
            // If in auto mode, start listening again after TTS
            const autoMode = document.getElementById('autoModeToggle').checked;
            if (autoMode) {
                recordAfterAudio();
            }
            // If in continuous recording mode, ensure we're listening
            if (continuousRecording) {
                logMessage("Continuing voice-activated recording...");
            }
        });

        // Handle completions ready event
        socket.on('completions_ready', (data) => {
            console.log('[DEBUG][completions_ready] Data:', data);
            // Show completions in the return boxes
            const completions = data.completions || [];
            // Clear all return boxes first
            document.getElementById('return1').value = "";
            document.getElementById('return2').value = "";
            document.getElementById('return3').value = "";
            document.getElementById('return4').value = "";
            // Fill each box with its respective completion
            if (completions.length > 0) {
                document.getElementById('return1').value = completions[0];
            }
            if (completions.length > 1) {
                document.getElementById('return2').value = completions[1];
            }
            if (completions.length > 2) {
                document.getElementById('return3').value = completions[2];
            }
            if (completions.length > 3) {
                document.getElementById('return4').value = completions[3];
            }
            // Update status
            logMessage("Completions ready. Select a response to continue.");
            // Remove auto-speak behavior - wait for user selection instead
        });

        // Handle pipeline status updates
        socket.on('pipeline_status', (data) => {
            console.log('[DEBUG][pipeline_status] Data:', data);
            // Update UI based on status
            if (data.status === 'completions_ready') {
                // Show completions in return boxes
                const completions = data.completions || [];
                // Clear all return boxes first
                document.getElementById('return1').value = "";
                document.getElementById('return2').value = "";
                document.getElementById('return3').value = "";
                document.getElementById('return4').value = "";
                // Fill each box with its respective completion
                if (completions.length > 0) document.getElementById('return1').value = completions[0] || "";
                if (completions.length > 1) document.getElementById('return2').value = completions[1] || "";
                if (completions.length > 2) document.getElementById('return3').value = completions[2] || "";
                if (completions.length > 3) document.getElementById('return4').value = completions[3] || "";
                // Show a status indicator
                document.getElementById('statusIndicator').textContent = "LLM responses received. Click on any blue response box to select it.";
                document.getElementById('statusIndicator').style.display = 'block';
                // Update transcript in the text input if provided
                if (data.transcript) {
                    document.getElementById('textInput').value = data.transcript;
                }
                // Remove auto-speak behavior - wait for user selection instead
            }
        });

        // Add this new event handler for updating the text box
        socket.on('update_textbox', (data) => {
            if (data.text) {
                document.getElementById('textInput').value = data.text;
            }
        });
        // Initialize Socket.IO connection
        function initializeSocketConnection() {
            if (socket && socket.connected) {
                console.log("Socket.IO already connected");
                return;
            }
            
            // Connect to the same host as the current page
            socket = io(window.location.origin);
            
            // Connection events
            socket.on('connect', () => {
                console.log('Connected to server via Socket.IO with ID:', socket.id);
            });
            
            socket.on('disconnect', () => {
                console.log('Disconnected from server');
            });
            
            // Status updates
            socket.on('status_update', (data) => {
                console.log('Status update:', data.status);
                logMessage(data.status);
            });
            
            // Transcript ready event
            socket.on('transcript_ready', (data) => {
                console.log('Transcript ready:', data.transcript);
                if (data.transcript) {
                    // Update textbox with transcript
                    if (lastBernardStatement) {
                        // SWAP ORDER: User first, then Bernard
                        document.getElementById('textInput').value = `User: ${data.transcript}\n\nBernard: ${lastBernardStatement}`;
                    } else {
                        document.getElementById('textInput').value = data.transcript;
                    }
                }
            });
            
            // LLM chunks events
            socket.on('llm_chunk', (data) => {
                console.log('LLM chunk received');
                
                // Show response in all return boxes if in auto mode
                const autoMode = document.getElementById('autoModeToggle').checked;
                if (autoMode) {
                    const responseText = data.full_response_so_far;
                    document.getElementById('return1').value = responseText;
                } else {
                    // For manual mode, show chunks in different return boxes
                    // Fill all return boxes with the response so far
                    document.getElementById('return1').value = data.full_response_so_far;
                }
            });
            
            // TTS chunks events
            socket.on('tts_chunk_ready', (data) => {
                console.log('TTS chunk ready:', data);
                
                // Add to audio queue for sequential playback
                if (data.audio_url) {
                    audioQueue.push({
                        url: data.audio_url,
                        index: data.chunk_index,
                        isLast: data.is_last || false
                    });
                    
                    // Start playing if not already playing
                    if (!isPlayingQueue) {
                        playNextChunkInQueue();
                    }
                }
            });
            
            // When pipeline is complete
            socket.on('pipeline_complete', (data) => {
                console.log('Pipeline complete:', data);
                
                // Store Bernard's statement for future context
                if (data.response) {
                    lastBernardStatement = data.response;
                }
                
                // Update UI to show completion
                logMessage("Processing complete. Ready for next input.");
                
                // If in auto mode, prepare for next recording after audio finishes
                // This is now handled by the audio queue player
            });
        }
        // Function to process audio using the parallel pipeline
        async function processAudioParallel(audioBlob) {
            const formData = new FormData();
            formData.append('audio', audioBlob);
            formData.append('session_id', socket.id);
            
            // Add the auto_mode flag based on the current toggle state
            const autoMode = document.getElementById('autoModeToggle').checked;
            formData.append('auto_mode', autoMode);
            
            try {
                const response = await fetch('/process_audio_parallel', {
                    method: 'POST',
                    body: formData
                });
                
                const data = await response.json();
                if (!data.success) {
                    console.error('Error starting parallel processing:', data.error);
                    return;
                }
                
                console.log('Parallel processing started:', data);
            } catch (error) {
                console.error('Error in processAudioParallel:', error);
            }
        }

        // Audio chunk playback queue
        const audioQueue = [];
        let isPlayingQueue = false;

        // Function to play audio chunks in sequence
        function playAudioChunk(url, chunkIndex, isLast = false) {
            // Add to queue
            audioQueue.push({
                url: url,
                index: chunkIndex,
                isLast: isLast
            });
            
            // Start playing if not already playing
            if (!isPlayingQueue) {
                playNextChunkInQueue();
            }
        }

        // Function to play the next chunk in the queue
        function playNextChunkInQueue() {
            if (audioQueue.length === 0) {
                isPlayingQueue = false;
                return;
            }
            
            isPlayingQueue = true;
            const chunk = audioQueue.shift();
            
            const audio = new Audio(chunk.url);
            
            audio.onended = () => {
                // Play next chunk when this one ends
                playNextChunkInQueue();
            };
            
            audio.onerror = (e) => {
                console.error('Error playing audio chunk:', e);
                // Continue to next chunk even if there's an error
                playNextChunkInQueue();
            };
            
            audio.play().catch(err => {
                console.error('Error playing audio:', err);
                playNextChunkInQueue();
            });
        }

        // Function to update response display
        function updateResponseDisplay(text) {
            // Update all response boxes with the text so far
            document.getElementById('return1').value = text;
        }

        // Function to cancel ongoing pipeline
        function cancelPipeline() {
            socket.emit('cancel_pipeline', { session_id: socket.id });
        }
        // Log messages to console and display in the systemMessages div
        function logMessage(msg) {
            console.log(msg);
            document.getElementById('systemMessages').innerHTML = msg;
        }
        
        // Helper function to strip prefixes
        function stripPrefixes(text) {
            // If the text has Bernard: and User: prefixes, extract just the user part
            if (text.includes("Bernard:") && text.includes("User:")) {
                const parts = text.split("User:");
                if (parts.length > 1) {
                    return parts[1].trim();
                }
            }
            return text;
        }
        
        // Function to play audio from URL and then run a callback when finished
        function playAudioWithCallback(url, callback) {
            const audioPlayer = document.getElementById('audioPlayer');
            
            // Ensure the URL is absolute
            if (url.startsWith('/')) {
                url = window.location.origin + url;
            }
            
            console.log("Playing audio from URL:", url);
            audioPlayer.src = url;
            audioPlayer.volume = 1.0; // Ensure volume is up
            audioPlayer.load(); // Ensure the new audio is loaded
            
            // Log when audio starts playing
            audioPlayer.onplay = () => {
                console.log("Audio playback started");
            };
            
            // Set up the ended event to trigger the callback
            audioPlayer.onended = () => {
                console.log("Audio playback ended");
                if (callback) {
                    callback();
                }
            };
            
            // Log errors
            audioPlayer.onerror = (e) => {
                console.error("Audio playback error:", e);
                logMessage("Error playing audio: " + (e.target.error ? e.target.error.message : "Unknown error"));
                if (callback) {
                    callback(); // Still try to run the callback on error
                }
            };
            
            // Play the audio
            const playPromise = audioPlayer.play();
            
            if (playPromise !== undefined) {
                playPromise
                    .then(() => {
                        console.log("Audio playback started successfully");
                    })
                    .catch(err => {
                        console.error("Error playing audio:", err);
                        logMessage("Audio blocked by browser. Please click anywhere to enable audio.");
                        // Add a one-time click handler to the document to play audio
                        const clickHandler = () => {
                            audioPlayer.play()
                                .then(() => {
                                    console.log("Audio playback started after user interaction");
                                })
                                .catch(err2 => {
                                    console.error("Error playing audio after user interaction:", err2);
                                    if (callback) {
                                        callback();
                                    }
                                });
                            document.removeEventListener('click', clickHandler);
                        };
                        document.addEventListener('click', clickHandler);
                        // If no user interaction within 5 seconds, just call the callback
                        setTimeout(() => {
                            if (callback) {
                                callback();
                            }
                        }, 5000);
                    });
            }
        }
        
    

        // Placeholder function for Avatar feature
        function avatarFunction() {
            console.log("avatarFunction() called");
            logMessage("Avatar functionality will be implemented at a later stage.");
        }

        // Placeholder function for Imagine feature
        function imagineFunction() {
            console.log("imagineFunction() called");
            logMessage("Imagine functionality will be implemented at a later stage.");
        }

        // Function to handle enhanced prompts
        async function enhancePrompt() {
            console.log("Prompt() called");
            const userText = document.getElementById('textInput').value;
            
            if (!userText.trim()) {
                logMessage("Please enter text before enhancing prompt");
                return;
            }
            
            logMessage("Enhancing prompt for Bernard...");
            
            try {
                const response = await fetch('/enhance_prompt', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        prompt: userText,
                        useContext: document.getElementById('contextToggle').checked,
                        useInternet: document.getElementById('internetToggle').checked
                    })
                });
                
                const data = await response.json();
                console.log("enhancePrompt response:", data);
                
                if (data.error) {
                    logMessage("Error: " + data.error);
                    return;
                }
                
                // Clear all return boxes first
                document.getElementById('return1').value = "";
                document.getElementById('return2').value = "";
                document.getElementById('return3').value = "";
                document.getElementById('return4').value = "";
                
                // Get the completions array
                const completions = data.completions || [];
                console.log("Received completions:", completions);
                
                // Fill each box with its respective completion
                // Make sure we're putting each unique response in its own box
                if (completions.length > 0) document.getElementById('return1').value = completions[0] || "";
                if (completions.length > 1) document.getElementById('return2').value = completions[1] || "";
                if (completions.length > 2) document.getElementById('return3').value = completions[2] || "";
                if (completions.length > 3) document.getElementById('return4').value = completions[3] || "";
                
                logMessage("Enhanced prompts received. Click on any blue response box to select it as Bernard's statement.");
            } catch (error) {
                console.error("Error in enhancePrompt:", error);
                logMessage("Error: " + error);
            }
        }
        function isAfterEnhancePrompt() {
            // Check if any of the return boxes have content (indicating enhance prompt was used)
            const hasReturnContent = 
                document.getElementById('return1').value.trim() !== "" || 
                document.getElementById('return2').value.trim() !== "" || 
                document.getElementById('return3').value.trim() !== "" || 
                document.getElementById('return4').value.trim() !== "";
            
            // Check if the text input has "Bernard:" prefix
            const textInput = document.getElementById('textInput').value;
            const hasBernardPrefix = textInput.trim().toLowerCase().startsWith("bernard:");
            
            // It's an enhanced prompt if both conditions are true
            return hasReturnContent && hasBernardPrefix;
        }
        // 5. Simple utility to record (helpful for debugging)
        function debugRecorderState() {
            if (!mediaRecorder) {
                return "No MediaRecorder initialized";
            }
            
            return {
                state: mediaRecorder.state,
                mimeType: mediaRecorder.mimeType,
                audioBitsPerSecond: mediaRecorder.audioBitsPerSecond,
                chunks: audioChunks.length,
                isRecording: isRecording
            };
        }
        
        async function retrieveContext() {
            console.log("retrieveContext() called");
            const query = document.getElementById('textInput').value;
            if (!query.trim()) {
                logMessage("Please enter text before retrieving context");
                return;
            }
            
            try {
                const response = await fetch('/retrieve_context', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ query })
                });
                const data = await response.json();
                console.log("retrieveContext response:", data);
                
                // Replace the textbox content with the retrieved context
                if (data.context) {
                    document.getElementById('textInput').value = data.context;
                    logMessage("Context retrieved and placed in textbox");
                } else {
                    logMessage("No relevant context found");
                }
            } catch (error) {
                console.error("Error in retrieveContext:", error);
                logMessage("Error: " + error);
            }
        }
        
        async function uploadDoc() {
            console.log("uploadDoc() called");
            // Create a temporary file input element
            const fileInput = document.createElement('input');
            fileInput.type = 'file';
            fileInput.accept = '.txt,.pdf';
            
            // Handle file selection
            fileInput.onchange = async (event) => {
                const file = event.target.files[0];
                if (!file) return;
                
                const formData = new FormData();
                formData.append('file', file);
                
                try {
                    const response = await fetch('/upload_doc', {
                        method: 'POST',
                        body: formData
                    });
                    const data = await response.json();
                    logMessage(data.message);
                } catch (error) {
                    logMessage("Error uploading document: " + error);
                }
            };
            
            // Trigger file selection dialog
            fileInput.click();
        }
        
        // Function to show log memory modal
        function showLogMemoryModal() {
            console.log("showLogMemoryModal() called");
            
            // Set today's date as default
            const today = new Date();
            const formattedDate = today.toISOString().split('T')[0]; // Format as YYYY-MM-DD
            document.getElementById('memoryDate').value = formattedDate;
            
            // If there's text in the main textbox, copy it to the memory text
            const textInput = document.getElementById('textInput').value.trim();
            if (textInput) {
                document.getElementById('memoryText').value = textInput;
            } else {
                document.getElementById('memoryText').value = '';
            }
            
            // Clear other fields
            document.getElementById('memoryTitle').value = '';
            document.getElementById('memoryTags').value = '';
            
            // Show the modal
            document.getElementById('logMemoryModal').style.display = 'block';
        }

        // Function to close log memory modal
        function closeLogMemoryModal() {
            document.getElementById('logMemoryModal').style.display = 'none';
        }

        // Function to save memory log
        async function saveMemoryLog() {
            console.log("saveMemoryLog() called");
            
            // Get values from form
            const title = document.getElementById('memoryTitle').value.trim();
            const tagsString = document.getElementById('memoryTags').value.trim();
            const dateString = document.getElementById('memoryDate').value;
            const text = document.getElementById('memoryText').value.trim();
            
            // Validate inputs
            if (!title) {
                alert("Please enter a title for the memory");
                return;
            }
            
            if (!text) {
                alert("Please enter text for the memory");
                return;
            }
            
            // Process tags - split by commas and trim whitespace
            const tags = tagsString ? tagsString.split(',').map(tag => tag.trim()) : [];
            
            // Create memory object
            const memory = {
                title: title,
                tags: tags,
                date: dateString,
                text: text
            };
            
            try {
                // Send to server
                const response = await fetch('/log_event', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        memory: memory
                    })
                });
                
                const data = await response.json();
                console.log("saveMemoryLog response:", data);
                
                if (data.error) {
                    alert("Error saving memory log: " + data.error);
                } else {
                    logMessage(data.message || "Memory logged successfully");
                    
                    // Close the modal
                    closeLogMemoryModal();
                    
                    // Clear the textbox
                    document.getElementById('textInput').value = "";
                }
            } catch (error) {
                console.error("Error in saveMemoryLog:", error);
                alert("Error saving memory log: " + error);
            }
        }
        
        async function updateVector() {
            console.log("updateVector() called");
            try {
                const response = await fetch('/update_vector', { method: 'POST' });
                const data = await response.json();
                console.log("updateVector response:", data);
                logMessage(data.message);
            } catch (error) {
                console.error("Error in updateVector:", error);
                logMessage("Error: " + error);
            }
        }
        
        function showSettings() {
            console.log("showSettings() called - to be implemented later");
            logMessage("Settings functionality will be implemented at a later stage.");
        }
        
        // Subject Modal Functions
        function showSubjectModal() {
            console.log("showSubjectModal() called");
            // Hide the subject form initially when opening the modal
            document.getElementById('subjectForm').style.display = 'none';
            loadSubjects();
            document.getElementById('subjectModal').style.display = 'block';
        }
        
        function closeSubjectModal() {
            document.getElementById('subjectModal').style.display = 'none';
            // Reset form state
            resetSubjectForm();
             // Reset conversation context when changing subjects
            lastBernardStatement = "";
        }
        
        function resetSubjectForm() {
            document.getElementById('subjectNameInput').value = '';
            document.getElementById('subjectDescriptionInput').value = '';
            document.getElementById('subjectForm').style.display = 'none';
            currentEditingSubject = null;
            isCreatingNewSubject = false;
        }
        
        async function loadSubjects() {
            try {
                // We'll load subjects from the server using a fetch call
                let subjects = [];
                try {
                    const response = await fetch('/get_subjects', { method: 'GET' });
                    const data = await response.json();
                    if (data.subjects) {
                        subjects = data.subjects;
                    }
                } catch (error) {
                    console.error("Error loading subjects:", error);
                    // If there's an error, use a default list
                    subjects = ["Default Subject"];
                }
                
                // If we still have no subjects, add a default one
                if (!subjects || subjects.length === 0) {
                    subjects = ["Default Subject"];
                    // Try to save this default subject
                    try {
                        await fetch('/select_subject', {
                            method: 'POST',
                            headers: {'Content-Type': 'application/json'},
                            body: JSON.stringify({ subject: "Default Subject" })
                        });
                    } catch (e) {
                        console.error("Error saving default subject:", e);}
                }
                
                const container = document.getElementById('subjectContainer');
                container.innerHTML = ''; // Clear existing subjects
                
                // Add each subject as a clickable rectangle
                subjects.forEach(subject => {
                    const subjectElement = document.createElement('div');
                    subjectElement.className = 'subject-item';
                    subjectElement.textContent = subject;
                    // Create a div to hold both select and edit actions
                    const actionsDiv = document.createElement('div');
                    actionsDiv.style.display = 'flex';
                    actionsDiv.style.gap = '5px';
                    
                    // Select action
                    subjectElement.onclick = () => selectSubject(subject);
                    
                    // Edit action (right-click or long-press)
                    subjectElement.oncontextmenu = (e) => {
                        e.preventDefault();
                        editSubject(subject);
                        return false;
                    };
                    
                    // Double-click to edit
                    subjectElement.ondblclick = (e) => {
                        e.preventDefault();
                        editSubject(subject);
                    };
                    
                    container.appendChild(subjectElement);
                });
                
                // Log what's happening
                console.log(`Loaded ${subjects.length} subjects:`, subjects);
                
                // Debug display of subjects
                if (subjects.length === 0) {
                    const emptyMessage = document.createElement('div');
                    emptyMessage.textContent = 'No subjects available. Please add one below.';
                    emptyMessage.style.color = '#666';
                    emptyMessage.style.padding = '10px';
                    container.appendChild(emptyMessage);
                }
            } catch (error) {
                console.error("Error in loadSubjects:", error);
                logMessage("Error loading subjects: " + error);
                
                // Add fallback message to container
                const container = document.getElementById('subjectContainer');
                container.innerHTML = '<div style="color: #666; padding: 10px;">Error loading subjects. Please try again.</div>';
            }
        }
        
        async function editSubject(subject) {
            console.log("editSubject() called for:", subject);
            currentEditingSubject = subject;
            isCreatingNewSubject = false;
            
            try {
                // Fetch the subject details
                const response = await fetch('/get_subject_details', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ subject: subject })
                });
                
                const data = await response.json();
                console.log("Subject details:", data);
                
                // Fill the form with the subject details
                document.getElementById('subjectNameInput').value = data.name || subject;
                document.getElementById('subjectNameInput').disabled = true; // Don't allow name changes for existing subjects
                document.getElementById('subjectDescriptionInput').value = data.description || '';
                
                // Show the form and update title
                document.getElementById('subjectFormTitle').textContent = `Edit Subject: ${subject}`;
                document.getElementById('subjectForm').style.display = 'block';
                
                // Focus on the description field
                document.getElementById('subjectDescriptionInput').focus();
            } catch (error) {
                console.error("Error fetching subject details:", error);
                logMessage("Error fetching subject details: " + error);
            }
        }
        
        function showNewSubjectForm() {
            console.log("showNewSubjectForm() called");
            
            // Reset and prepare form
            document.getElementById('subjectNameInput').value = '';
            document.getElementById('subjectNameInput').disabled = false;
            document.getElementById('subjectDescriptionInput').value = '';
            document.getElementById('subjectFormTitle').textContent = 'Add New Subject';
            
            // Show the form
            document.getElementById('subjectForm').style.display = 'block';
            
            // Set flags
            isCreatingNewSubject = true;
            currentEditingSubject = null;
            
            // Focus on the name field
            document.getElementById('subjectNameInput').focus();
        }
        
        function cancelSubjectEdit() {
            resetSubjectForm();
        }
        
        async function saveSubjectDetails() {
            console.log("saveSubjectDetails() called");
            
            // Get form values
            const name = document.getElementById('subjectNameInput').value.trim();
            const description = document.getElementById('subjectDescriptionInput').value.trim();
            
            if (!name) {
                alert("Subject name is required");
                return;
            }
            
            try {
                // Check for duplicate if this is a new subject
                if (isCreatingNewSubject) {
                    const checkResponse = await fetch('/get_subjects', { method: 'GET' });
                    const checkData = await checkResponse.json();
                    
                    if (checkData.subjects && checkData.subjects.some(subject => 
                        subject.toLowerCase() === name.toLowerCase() && name.toLowerCase() !== (currentEditingSubject || '').toLowerCase())) {
                        alert("This subject already exists. Please enter a unique subject name.");
                        return;
                    }
                }
                
                // Save the subject with description
                const response = await fetch('/update_subject', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        name: isCreatingNewSubject ? name : currentEditingSubject,
                        description: description
                    })
                });
                
                const data = await response.json();
                console.log("saveSubjectDetails response:", data);
                
                if (data.error) {
                    alert("Error saving subject: " + data.error);
                } else {
                    logMessage(data.message);
                    
                    // Close the form and reset
                    resetSubjectForm();
                    
                    // Reload the subjects list
                    loadSubjects();
                    
                    // If this is a new subject, automatically select it
                    if (isCreatingNewSubject) {
                        await selectSubject(name);
                    }
                }
            } catch (error) {
                console.error("Error in saveSubjectDetails:", error);
                alert("Error saving subject: " + error);
            }
        }
        
        async function selectSubject(subject) {
            console.log("selectSubject() called for:", subject);
            
            // Get the subject description before selecting
            try {
                const detailsResponse = await fetch('/get_subject_details', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ subject: subject })
                });
                
                const details = await detailsResponse.json();
                const description = details.description || "";
                
                // Now select the subject with the description
                const response = await fetch('/select_subject', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        subject: subject,
                        description: description
                    })
                });
                
                const data = await response.json();
                console.log("selectSubject response:", data);
                logMessage(data.message);
                
                // Update the subject button text and color
                const subjectButton = document.getElementById('subjectButton');
                subjectButton.textContent = subject.toUpperCase();
                subjectButton.style.backgroundColor = '#2ecc71'; // Green color
                
                // Close the modal
                closeSubjectModal();
            } catch (error) {
                console.error("Error in selectSubject:", error);
                logMessage("Error: " + error);
            }
        }
        
        // First, let's create a WAV encoder utility to ensure server compatibility
        const createWavFromAudioBuffer = (audioBuffer, sampleRate) => {
            // Convert Float32Array to Int16Array for WAV compatibility
            const interleaved = new Int16Array(audioBuffer.length);
            
            // Normalize and convert to 16-bit PCM
            for (let i = 0; i < audioBuffer.length; i++) {
                // Clamp between -1 and 1
                const sample = Math.max(-1, Math.min(1, audioBuffer[i]));
                // Convert to 16-bit signed integer
                interleaved[i] = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
            }
            
            // Create WAV file header
            const buffer = new ArrayBuffer(44 + interleaved.length * 2);
            const view = new DataView(buffer);
            
            // Write WAV header
            // "RIFF" chunk descriptor
            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + interleaved.length * 2, true);
            writeString(view, 8, 'WAVE');
            
            // "fmt " sub-chunk
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);               // Subchunk1Size (16 for PCM)
            view.setUint16(20, 1, true);                // AudioFormat (1 for PCM)
            view.setUint16(22, 1, true);                // NumChannels (1 for mono)
            view.setUint32(24, sampleRate, true);       // SampleRate
            view.setUint32(28, sampleRate * 2, true);   // ByteRate (SampleRate * NumChannels * BitsPerSample/8)
            view.setUint16(32, 2, true);                // BlockAlign (NumChannels * BitsPerSample/8)
            view.setUint16(34, 16, true);               // BitsPerSample (16 bits)
            
            // "data" sub-chunk
            writeString(view, 36, 'data');
            view.setUint32(40, interleaved.length * 2, true); // Subchunk2Size
            
            // Write audio data
            for (let i = 0; i < interleaved.length; i++) {
                view.setInt16(44 + i * 2, interleaved[i], true);
            }
            
            return new Blob([view], { type: 'audio/wav' });
        };

        // Helper function to write string to DataView
        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }
        // 1. Add a function to fetch recorder settings from the server
        async function getRecorderSettings() {
            try {
                const response = await fetch('/get_settings', { method: 'GET' });
                const data = await response.json();
                
                if (data && data.recorder) {
                    console.log("Retrieved recorder settings from server:", data.recorder);
                    return data.recorder;
                } else {
                    console.warn("No recorder settings found, using defaults");
                    return {
                        silenceThreshold: 500,
                        silenceDuration: 2,
                        minRecordingDuration: 1,
                        maxRecordingDuration: 60,
                        useNoiseReduction: true
                    };
                }
            } catch (error) {
                console.error("Error fetching recorder settings:", error);
                // Return defaults if we can't get settings
                return {
                    silenceThreshold: 500,
                    silenceDuration: 2,
                    minRecordingDuration: 1,
                    maxRecordingDuration: 60,
                    useNoiseReduction: true
                };
            }
        }

        // 2. Modified startClientRecording function to use server settings
        async function startClientRecording(isManual = false) {
            console.log("[DEBUG] startClientRecording called");
            try {
                const recorderSettings = await getRecorderSettings();
                console.log("[DEBUG] Recorder settings:", recorderSettings);
                audioChunks = [];
                rawAudioChunks = [];
                isRecording = true;
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: recorderSettings.useNoiseReduction,
                        noiseSuppression: recorderSettings.useNoiseReduction,
                        channelCount: 1,
                        sampleRate: 16000
                    } 
                });
                const audioTracks = stream.getAudioTracks();
                const trackSettings = audioTracks[0].getSettings();
                actualSampleRate = trackSettings.sampleRate || 44100;
                console.log(`[DEBUG] Actual sample rate: ${actualSampleRate}Hz`);
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: actualSampleRate
                });
                if (!socket || !socket.connected) {
                    initializeSocketConnection();
                }
                const source = audioContext.createMediaStreamSource(stream);
                analyzer = audioContext.createAnalyser();
                analyzer.fftSize = 2048;
                source.connect(analyzer);
                const scriptProcessor = audioContext.createScriptProcessor(4096, 1, 1);
                source.connect(scriptProcessor);
                scriptProcessor.connect(audioContext.destination);
                scriptProcessor.onaudioprocess = (audioProcessingEvent) => {
                    if (!isRecording) return;
                    const inputBuffer = audioProcessingEvent.inputBuffer;
                    const inputData = inputBuffer.getChannelData(0);
                    const audioData = new Float32Array(inputData.length);
                    audioData.set(inputData);
                    rawAudioChunks.push(audioData);
                };
                silenceThreshold = (recorderSettings.silenceThreshold > 0) ? Math.min(0.1, 50 / recorderSettings.silenceThreshold) : 0.1;
                silenceDuration = recorderSettings.silenceDuration * 1000;
                const minRecordingDuration = recorderSettings.minRecordingDuration * 1000;
                const maxRecordingDuration = recorderSettings.maxRecordingDuration * 1000;
                console.log(`[DEBUG] Silence settings - threshold: ${silenceThreshold}, duration: ${silenceDuration}ms, min: ${minRecordingDuration}ms, max: ${maxRecordingDuration}ms`);
                let mimeType = '';
                if (MediaRecorder.isTypeSupported('audio/webm')) {
                    mimeType = 'audio/webm';
                } else if (MediaRecorder.isTypeSupported('audio/ogg')) {
                    mimeType = 'audio/ogg';
                }
                console.log("[DEBUG] Using MediaRecorder MIME type:", mimeType || "browser default");
                const options = mimeType ? { mimeType: mimeType, audioBitsPerSecond: 128000 } : {};
                mediaRecorder = new MediaRecorder(stream, options);
                mediaRecorder.addEventListener('dataavailable', (event) => {
                    if (event.data && event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                });
                mediaRecorder.addEventListener('stop', () => {
                    console.log("[DEBUG] MediaRecorder stopped");
                    if (scriptProcessor) {
                        scriptProcessor.disconnect();
                    }
                    processRawAudio(rawAudioChunks, actualSampleRate);
                });
                mediaRecorder.start(1000);
                recordingStartTime = Date.now();
                lastAudioTime = Date.now();
                logMessage("Recording started. Speak now or press STOP RECORDING when finished.");
                // Add debug log for silence detection interval
                console.log('[DEBUG] Setting up silenceDetectionInterval');
                silenceDetectionInterval = setInterval(() => {
                    detectSilence(minRecordingDuration, maxRecordingDuration);
                }, 250);
                // Log analyzer and audioContext objects
                console.log('[DEBUG] analyzer object:', analyzer);
                console.log('[DEBUG] audioContext object:', audioContext);
                return true;
            } catch (error) {
                console.error("[DEBUG] Error starting client recording:", error);
                logMessage("Error accessing microphone: " + error.message);
                fallbackToServerRecording();
                return false;
            }
        }

        // 3. Updated silence detection function to use the passed parameters
        function detectSilence(minRecordingDuration, maxRecordingDuration) {
            console.log('[DEBUG] Entered detectSilence');
            if (!analyzer || !isRecording) return;
            const dataArray = new Uint8Array(analyzer.frequencyBinCount);
            analyzer.getByteFrequencyData(dataArray);
            let sum = 0;
            for (let i = 0; i < dataArray.length; i++) {
                sum += dataArray[i];
            }
            const avg = sum / dataArray.length / 255; // Normalize to 0-1
            // Add detailed debug log for audio level
            console.log(`[DEBUG] detectSilence: avg audio level = ${avg.toFixed(4)}, threshold = ${silenceThreshold}`);
            if (avg > silenceThreshold) {
                lastAudioTime = Date.now();
                console.log('[DEBUG] Audio above threshold, resetting lastAudioTime');
            }
            const silenceTime = Date.now() - lastAudioTime;
            const recordingDuration = Date.now() - recordingStartTime;
            // Add debug log for silence and recording duration
            console.log(`[DEBUG] silenceTime = ${silenceTime}ms, silenceDuration = ${silenceDuration}ms, recordingDuration = ${recordingDuration}ms, min = ${minRecordingDuration}ms, max = ${maxRecordingDuration}ms`);
            if (silenceTime > silenceDuration && recordingDuration > minRecordingDuration) {
                console.log(`[DEBUG] Silence detected for ${silenceTime/1000}s (threshold: ${silenceDuration/1000}s), stopping automatically`);
                stopClientRecording(true);
            }
            if (recordingDuration > maxRecordingDuration) {
                console.log(`[DEBUG] Maximum recording duration reached (${recordingDuration/1000}s of ${maxRecordingDuration/1000}s)`);
                stopClientRecording(true);
            }
        }

        // 3. Process raw audio into a WAV file compatible with server's transcription
        function processRawAudio(rawChunks, sampleRate) {
            console.log(`[DEBUG] processRawAudio: rawChunks.length = ${rawChunks.length}`);
            let totalLength = 0;
            for (const chunk of rawChunks) {
                totalLength += chunk.length;
            }
            console.log(`[DEBUG] processRawAudio: totalLength = ${totalLength}`);
            if (!rawChunks || rawChunks.length === 0) {
                console.error("[DEBUG] No raw audio chunks to process");
                if (audioChunks && audioChunks.length > 0) {
                    sendAudioForTranscription(new Blob(audioChunks));
                } else {
                    logMessage("No audio was captured.");
                    if (continuousRecording) {
                        logMessage("Continuing voice-activated recording...");
                    } else {
                        const autoMode = document.getElementById('autoModeToggle').checked;
                        if (autoMode) {
                            startRecording();
                        }
                    }
                }
                return;
            }
            try {
                let totalLength = 0;
                for (const chunk of rawChunks) {
                    totalLength += chunk.length;
                }
                const combinedAudio = new Float32Array(totalLength);
                let position = 0;
                for (const chunk of rawChunks) {
                    combinedAudio.set(chunk, position);
                    position += chunk.length;
                }
                const wavBlob = createWavFromAudioBuffer(combinedAudio, sampleRate);
                console.log(`[DEBUG] Created WAV blob: ${wavBlob.size} bytes`);
                // Manual recording file size check
                if (manualRecordingTimer !== null && wavBlob.size > MANUAL_MAX_FILE_SIZE) {
                    logMessage("Recording too long or too large (over 30MB). Please try again with a shorter message.");
                    if (manualRecordingTimer) { clearTimeout(manualRecordingTimer); manualRecordingTimer = null; }
                    stopClientRecording(false);
                    return;
                }
                sendAudioForTranscription(wavBlob);
            } catch (error) {
                console.error("[DEBUG] Error processing raw audio:", error);
                if (audioChunks && audioChunks.length > 0) {
                    sendAudioForTranscription(new Blob(audioChunks));
                } else {
                    logMessage("Error creating audio file: " + error.message);
                    if (continuousRecording) {
                        logMessage("Continuing voice-activated recording...");
                    } else {
                        const autoMode = document.getElementById('autoModeToggle').checked;
                        if (autoMode) {
                            startRecording();
                        }
                    }
                }
            }
        }

        async function processAudioParallel(audioBlob, sampleRate = actualSampleRate) {
            console.log("[DEBUG] processAudioParallel called, blob size:", audioBlob.size, "sampleRate:", sampleRate);
            const formData = new FormData();
            formData.append('audio', audioBlob);
            formData.append('session_id', socket.id);
            const autoMode = document.getElementById('autoModeToggle').checked;
            formData.append('auto_mode', autoMode);
            try {
                const response = await fetch('/process_audio_parallel', {
                    method: 'POST',
                    body: formData
                });
                const data = await response.json();
                console.log("[DEBUG] processAudioParallel response:", data);
                if (!data.success) {
                    console.error('[DEBUG] Error starting parallel processing:', data.error);
                    logMessage("Error processing audio: " + (data.error || "Unknown error"));
                    return;
                }
                console.log('[DEBUG] Parallel processing started:', data);
            } catch (error) {
                console.error("[DEBUG] Error processing audio for parallel pipeline:", error);
                logMessage("Error processing audio: " + error.message);
            }
        }

        // 5. Stop recording function
        async function stopClientRecording(autoStopped = false) {
            console.log(`[DEBUG] stopClientRecording called, autoStopped: ${autoStopped}`);
            if (!isRecording) {
                console.log('[DEBUG] No active recording to stop.');
                logMessage(autoStopped ? "Recording already stopped automatically." : "No active recording to stop.");
                return;
            }
            isRecording = false;
            logMessage(autoStopped ? "Recording auto-stopped after silence detected." : "Stopping recording and processing transcript...");
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                try {
                    mediaRecorder.stop();
                    console.log('[DEBUG] MediaRecorder stopped by stopClientRecording');
                } catch (error) {
                    console.error('[DEBUG] Error stopping MediaRecorder:', error);
                }
            }
            clearInterval(silenceDetectionInterval);
            silenceDetectionInterval = null;
            if (audioChunks && audioChunks.length > 0) {
                const webmBlob = new Blob(audioChunks, { type: 'audio/webm' });
                processAudioParallel(webmBlob, actualSampleRate);
                audioChunks = [];
                rawAudioChunks = [];
                return;
            }
        }

        // Modified function to send audio to server and auto-trigger sendLLM
        async function sendAudioForTranscription(audioBlob) {
            console.log(`[DEBUG] sendAudioForTranscription: blob size = ${audioBlob.size} bytes, type = ${audioBlob.type}`);
            if (audioBlob.size > MANUAL_MAX_FILE_SIZE) {
                showError('Recording too large (max 30 MB). Please try again.');
                return;
            }
            try {
                console.log(`Sending audio to server: ${audioBlob.size} bytes, type: ${audioBlob.type}`);
                logMessage("Processing audio... Please wait...");
                
                // Create a FormData object
                const formData = new FormData();
                
                // Make sure we're sending a WAV file for best compatibility
                const filename = `recording_${Date.now()}.wav`;
                formData.append('audio', audioBlob, filename);
                
                // Send to server
                const response = await fetch('/transcribe_audio', {
                    method: 'POST',
                    body: formData
                });
                
                if (!response.ok) {
                    throw new Error(`Server error: ${response.status} ${response.statusText}`);
                }
                
                const data = await response.json();
                console.log("Transcription response:", data);
                
                // Update UI with transcript
                if (data.transcript) {
                    // Clean up transcript - remove content within parentheses
                    const cleanedTranscript = cleanupTranscript(data.transcript);
                    
                    logMessage("Transcript: " + cleanedTranscript);
                    
                    // Format with Bernard/User if in conversation flow
                    const displayText = lastBernardStatement 
                        ? `Bernard: ${lastBernardStatement}\n\nUser: ${cleanedTranscript}` 
                        : cleanedTranscript;
                    
                    // Update the textbox
                    document.getElementById('textInput').value = displayText;
                    
                    // Check if auto mode is enabled
                    const autoMode = document.getElementById('autoModeToggle').checked;
                    if (autoMode) {
                        logMessage("Auto mode: Automatically sending transcript to LLM...");
                        await sendLLM();
                    } else {
                        // NEW CODE: Automatically trigger sendLLM to populate the reply boxes
                        // but don't auto-select a response
                        logMessage("Getting LLM responses...");
                        await sendLLM();
                        if (continuousRecording) {
                            logMessage("Continuing voice-activated recording...");
                        } else {
                            const autoMode = document.getElementById('autoModeToggle').checked;
                            if (autoMode) {
                                // Start a new recording if in auto mode
                                startRecording();
                            }
                        }
                    }
                } else {
                    logMessage("No transcript obtained from recording.");
                    if (continuousRecording) {
                        logMessage("Continuing voice-activated recording...");
                    } else {
                        const autoMode = document.getElementById('autoModeToggle').checked;
                        if (autoMode) {
                            // Start a new recording if in auto mode
                            startRecording();
                        }
                    }
                }
            } catch (error) {
                console.error("Error sending audio for transcription:", error);
                logMessage("Error processing audio: " + error.message);
                if (continuousRecording) {
                    logMessage("Continuing voice-activated recording...");
                } else {
                    const autoMode = document.getElementById('autoModeToggle').checked;
                    if (autoMode) {
                        // Start a new recording if in auto mode
                        startRecording();
                    }
                }
            }
        }

        // Function to clean up transcript by removing content within parentheses
        function cleanupTranscript(transcript) {
            if (!transcript) return "";
            
            // Remove all content within parentheses
            const cleanedTranscript = transcript.replace(/\([^)]*\)/g, '');
            
            // Clean up any double spaces that might result from removing parenthetical content
            return cleanedTranscript.replace(/\s+/g, ' ').trim();
        }
        
        // 7. Updated recordAfterAudio function
        async function recordAfterAudio() {
            console.log("[DEBUG] recordAfterAudio called");
            if (isRecording) {
                console.log("[DEBUG] Already recording, stopping first");
                stopClientRecording(false);
            }
            audioChunks = [];
            rawAudioChunks = [];
            await new Promise(resolve => setTimeout(resolve, 300));
            logMessage("Audio finished playing. Now listening...");
            await startClientRecording();
        }

        // Fallback to server-side recording if client-side fails
        async function fallbackToServerRecording() {
            console.log("Falling back to server-side recording");
            logMessage("Falling back to server-side recording...");
            
            try {
                const response = await fetch('/start_recording', { method: 'POST' });
                const data = await response.json();
                
                if (data.success) {
                    logMessage("Server-side recording started. Please speak now.");
                    
                    // Start polling to check when recording has completed
                    recordingCheckInterval = setInterval(checkRecordingStatus, 1000);
                } else {
                    logMessage("Failed to start server recording: " + (data.message || "Unknown error"));
                }
            } catch (error) {
                console.error("Error starting server recording:", error);
                logMessage("Failed to start server recording: " + error.message);
            }
        }

        // Completely revamped startRecording function that doesn't rely on wake word
        async function startRecording() {
            console.log("startRecording() called with voice activation");
            
            // Toggle continuous recording mode
            continuousRecording = !continuousRecording;
            
            if (continuousRecording) {
                // Change button color to indicate active continuous recording
                const recordingContainer = document.querySelector('.button-split-container:nth-of-type(2)');
                const startButton = recordingContainer.querySelector('.split-button:first-child');
                startButton.style.backgroundColor = '#e74c3c'; // Red to show active recording
                startButton.textContent = 'STOP CONTINUOUS';
                
                // Clear text input if this is a new conversation
                if (!lastBernardStatement) {
                    document.getElementById('textInput').value = "";
                }
                
                logMessage("Continuous voice-activated recording started. Speak anytime.");
                
                // Start recording in voice-activated mode
                await startVoiceActivatedRecording();
            } else {
                // Stop continuous recording
                await stopContinuousRecording();
            }
        }

        // New function to stop continuous recording
        async function stopContinuousRecording() {
            console.log("stopContinuousRecording() called");
            
            // Revert button appearance
            const recordingContainer = document.querySelector('.button-split-container:nth-of-type(2)');
            const startButton = recordingContainer.querySelector('.split-button:first-child');
            startButton.style.backgroundColor = '#1a5276'; // Default blue
            startButton.textContent = 'START AUTO RECORDING';
            
            // Process any remaining audio if we have raw audio chunks
            if (rawAudioChunks && rawAudioChunks.length > 0) {
                console.log("Processing final audio segment before stopping");
                logMessage("Processing final speech segment...");
                
                try {
                    // Create a copy of the current chunks before resetting
                    const currentChunks = [...rawAudioChunks];
                    
                    // Combine all audio chunks into one Float32Array
                    let totalLength = 0;
                    for (const chunk of currentChunks) {
                        totalLength += chunk.length;
                    }
                    
                    const combinedAudio = new Float32Array(totalLength);
                    let position = 0;
                    
                    for (const chunk of currentChunks) {
                        combinedAudio.set(chunk, position);
                        position += chunk.length;
                    }
                    
                    // Create WAV file from the audio buffer (only if audioContext exists)
                    if (audioContext) {
                        const wavBlob = createWavFromAudioBuffer(combinedAudio, audioContext.sampleRate);
                        
                        // Instead of sendAudioForTranscription, use parallel pipeline
                        const formData = new FormData();
                        formData.append('audio', wavBlob, 'final_segment.wav');
                        if (socket && socket.connected) {
                            formData.append('session_id', socket.id);
                        } else {
                            formData.append('session_id', 'final_' + Date.now());
                        }
                        
                        // Send to the parallel pipeline endpoint
                        const response = await fetch('/process_audio_parallel', {
                            method: 'POST',
                            body: formData
                        });
                        
                        const data = await response.json();
                        if (!data.success) {
                            console.error('Error starting parallel processing:', data.error);
                            logMessage("Error processing final speech: " + (data.error || "Unknown error"));
                        }
                    }
                } catch (error) {
                    console.error("Error processing final audio segment:", error);
                    logMessage("Error processing final speech: " + error.message);
                }
            }
            
            // Stop active recording
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                stopClientRecording(false);
            }
            
            // Clear any pending silence detection
            if (silenceDetectionTimeout) {
                clearTimeout(silenceDetectionTimeout);
                silenceDetectionTimeout = null;
            }
            
            // Close any open audio contexts
            if (audioContext && audioContext.state !== 'closed') {
                try {
                    await audioContext.close();
                } catch (err) {
                    console.error("Error closing audio context:", err);
                }
            }
            
            // Stop the media stream tracks
            if (voiceActivationStream) {
                voiceActivationStream.getTracks().forEach(track => track.stop());
                voiceActivationStream = null;
            }
            
            // Cancel any ongoing Socket.IO operations
            if (socket && socket.connected) {
                socket.emit('cancel_pipeline', { session_id: socket.id });
            }
            
            // Reset all flags and buffers
            continuousRecording = false;
            voiceActivationEnabled = false;
            isProcessingTranscript = false;
            voiceDetected = false;
            rawAudioChunks = [];
            audioChunks = [];
            
            // Reset any queued audio
            audioQueue.length = 0;
            isPlayingQueue = false;
            
            logMessage("Continuous voice-activated recording stopped.");
        }

        // New function to handle voice-activated recording without wake word
        async function startVoiceActivatedRecording() {
            if (!continuousRecording) return;
            console.log('[DEBUG] startVoiceActivatedRecording called');
            voiceActivationEnabled = true;
            audioData = [];
            isProcessingTranscript = false;
            try {
                const recorderSettings = await getRecorderSettings();
                audioChunks = [];
                rawAudioChunks = [];
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: recorderSettings.useNoiseReduction,
                        noiseSuppression: recorderSettings.useNoiseReduction,
                        channelCount: 1,
                        sampleRate: 16000
                    } 
                });
                voiceActivationStream = stream;
                const audioTracks = stream.getAudioTracks();
                const trackSettings = audioTracks[0].getSettings();
                actualSampleRate = trackSettings.sampleRate || 44100;
                if (audioContext && audioContext.state !== 'closed') {
                    try {
                        await audioContext.close();
                    } catch (err) {
                        console.error("Error closing previous audio context:", err);
                    }
                }
                if (!socket || !socket.connected) {
                    initializeSocketConnection();
                }
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: actualSampleRate
                });
                const source = audioContext.createMediaStreamSource(stream);
                analyzer = audioContext.createAnalyser();
                analyzer.fftSize = 2048;
                source.connect(analyzer);
                const scriptProcessor = audioContext.createScriptProcessor(4096, 1, 1);
                source.connect(scriptProcessor);
                scriptProcessor.connect(audioContext.destination);
                // Log analyzer and audioContext objects
                console.log('[DEBUG] analyzer object (auto):', analyzer);
                console.log('[DEBUG] audioContext object (auto):', audioContext);
                // Voice detection variables
                let voiceDetected = false;
                let voiceActiveStartTime = 0;
                let lastVoiceTime = 0;
                const voiceThreshold = recorderSettings.silenceThreshold > 0 ? 
                                      Math.min(0.1, 50 / recorderSettings.silenceThreshold) : 0.1;
                const silenceDurationMs = recorderSettings.silenceDuration * 1000;
                const minRecordingTimeMs = recorderSettings.minRecordingDuration * 1000;
                scriptProcessor.onaudioprocess = (audioProcessingEvent) => {
                    if (!isRecording || !voiceActivationEnabled) return;
                    const inputBuffer = audioProcessingEvent.inputBuffer;
                    const inputData = inputBuffer.getChannelData(0);
                    // Calculate RMS for better voice detection
                    let sumSquares = 0;
                    for (let i = 0; i < inputData.length; i++) {
                        sumSquares += inputData[i] * inputData[i];
                    }
                    const rms = Math.sqrt(sumSquares / inputData.length);
                    // Debug log for audio level
                    console.log(`[DEBUG] (auto) onaudioprocess: rms = ${rms.toFixed(4)}, threshold = ${voiceThreshold}`);
                    if (rms > voiceThreshold) {
                        if (!voiceDetected) {
                            console.log('[DEBUG] (auto) Voice activity started');
                            voiceDetected = true;
                            voiceActiveStartTime = Date.now();
                            if (silenceDetectionTimeout) {
                                clearTimeout(silenceDetectionTimeout);
                                silenceDetectionTimeout = null;
                            }
                        }
                        lastVoiceTime = Date.now();
                        const audioData = new Float32Array(inputData.length);
                        audioData.set(inputData);
                        rawAudioChunks.push(audioData);
                    } else if (voiceDetected) {
                        const audioData = new Float32Array(inputData.length);
                        audioData.set(inputData);
                        rawAudioChunks.push(audioData);
                        const silenceTime = Date.now() - lastVoiceTime;
                        const recordingDuration = Date.now() - voiceActiveStartTime;
                        // Debug log for silence and recording duration
                        console.log(`[DEBUG] (auto) silenceTime = ${silenceTime}ms, silenceDuration = ${silenceDurationMs}ms, recordingDuration = ${recordingDuration}ms, min = ${minRecordingTimeMs}ms`);
                        if (silenceTime > silenceDurationMs && 
                            recordingDuration > minRecordingTimeMs && 
                            !isProcessingTranscript && 
                            !silenceDetectionTimeout) {
                            console.log(`[DEBUG] (auto) Silence detected for ${silenceTime/1000}s after ${recordingDuration/1000}s of speech`);
                            silenceDetectionTimeout = setTimeout(() => {
                                if (rawAudioChunks.length > 10) {
                                    console.log('[DEBUG] (auto) Processing voice segment after silence');
                                    processVoiceSegmentParallel();
                                } else {
                                    console.log('[DEBUG] (auto) Ignoring too-short recording segment');
                                }
                                voiceDetected = false;
                                rawAudioChunks = [];
                                silenceDetectionTimeout = null;
                            }, 300);
                        }
                        if (voiceDetected) {
                            const recordingDuration = Date.now() - voiceActiveStartTime;
                            if (recordingDuration > recorderSettings.maxRecordingDuration * 1000) {
                                console.log(`[DEBUG] (auto) Maximum recording duration reached (${recordingDuration/1000}s of ${recorderSettings.maxRecordingDuration}s)`);
                                clearTimeout(silenceDetectionTimeout);
                                silenceDetectionTimeout = setTimeout(() => {
                                    console.log('[DEBUG] (auto) Processing voice segment after max duration');
                                    processVoiceSegmentParallel();
                                    voiceDetected = false;
                                    rawAudioChunks = [];
                                }, 300);
                            }
                        }
                    }
                };
                logMessage("Voice activation ready. Speak anytime.");
                isRecording = true;
            } catch (error) {
                console.error("Error starting voice-activated recording:", error);
                logMessage("Error accessing microphone: " + error.message);
                await stopContinuousRecording();
            }
        }

        // Updated processVoiceSegment function to avoid duplicate processing
        async function processVoiceSegment() {
            if (!continuousRecording) return;
            
            // Add to queue instead of processing immediately
            const currentChunks = [...rawAudioChunks];
            transcriptQueue.push(currentChunks);
            rawAudioChunks = [];
            
            // Process queue if not already processing
            if (!processingQueue) {
                processTranscriptQueue();
            }
        }

        async function processChunks(chunks) {
            try {
                if (chunks && chunks.length > 0) {
                    // Combine all audio chunks into one Float32Array
                    let totalLength = 0;
                    for (const chunk of chunks) {
                        totalLength += chunk.length;
                    }
                    
                    const combinedAudio = new Float32Array(totalLength);
                    let position = 0;
                    
                    for (const chunk of chunks) {
                        combinedAudio.set(chunk, position);
                        position += chunk.length;
                    }
                    
                    // Create WAV file from the audio buffer
                    const wavBlob = createWavFromAudioBuffer(combinedAudio, audioContext.sampleRate);
                    
                    // Send for transcription
                    await sendSegmentForTranscription(wavBlob);
                }
            } catch (error) {
                console.error("Error processing chunks:", error);
                logMessage("Error processing speech: " + error.message);
            }
        }

        // New function to process the queue
        async function processTranscriptQueue() {
            if (processingQueue || transcriptQueue.length === 0) return;
            
            processingQueue = true;
            
            while (transcriptQueue.length > 0 && activeTranscriptions < MAX_CONCURRENT_TRANSCRIPTIONS) {
                const chunks = transcriptQueue.shift();
                activeTranscriptions++;
                
                // Process in parallel
                processChunks(chunks).finally(() => {
                    activeTranscriptions--;
                    if (transcriptQueue.length > 0) {
                        processTranscriptQueue();
                    }
                });
            }
            
            processingQueue = false;
        }

        // Add this new function after other functions
        function handleNewTranscript(transcript) {
            // Add to conversation history
            currentConversation.exchanges.push({
                user: transcript,
                bernard: lastBernardStatement || ""
            });
            
            // Limit exchanges to last 10 for memory management
            if (currentConversation.exchanges.length > 10) {
                currentConversation.exchanges.shift();
            }
            
            currentConversation.lastUserInput = transcript;
            
            // Only display the current exchange in the textbox
            const conversationText = lastBernardStatement
                ? `Bernard: ${lastBernardStatement}\n\nUser: ${transcript}`
                : transcript;
            
            document.getElementById('textInput').value = conversationText;
            
            // Queue LLM request with just the current exchange
            queueLLMRequest(transcript);
        }

        // Send a segment for transcription
        async function sendSegmentForTranscription(audioBlob) {
            try {
                console.log(`Sending speech segment: ${audioBlob.size} bytes, type: ${audioBlob.type}`);
                
                // Create a FormData object
                const formData = new FormData();
                const filename = `segment_${Date.now()}.wav`;
                formData.append('audio', audioBlob, filename);
                
                // Send to server
                const response = await fetch('/transcribe_audio', {
                    method: 'POST',
                    body: formData
                });
                
                if (!response.ok) {
                    throw new Error(`Server error: ${response.status} ${response.statusText}`);
                }
                
                const data = await response.json();
                console.log("Raw transcription response:", data);
                
                // Update UI with transcript
                if (data.transcript) {
                    // Clean up transcript - remove content within parentheses
                    const cleanedTranscript = cleanupTranscript(data.transcript);
                    
                    if (cleanedTranscript) {
                        logMessage("Transcript: " + cleanedTranscript);
                        handleNewTranscript(cleanedTranscript);
                    } else {
                        console.log("Transcript was too short or invalid");
                    }
                } else {
                    console.log("No valid transcript obtained");
                }
            } catch (error) {
                console.error("Error processing transcript:", error);
                logMessage("Error processing speech: " + error.message);
            }
        }

        async function processVoiceSegmentParallel() {
            if (!continuousRecording) return;
            
            isProcessingTranscript = true;
            
            try {
                // Make a copy of the chunks for processing
                const currentChunks = [...rawAudioChunks];
                
                // Combine all audio chunks into one Float32Array
                let totalLength = 0;
                for (const chunk of currentChunks) {
                    totalLength += chunk.length;
                }
                
                const combinedAudio = new Float32Array(totalLength);
                let position = 0;
                
                for (const chunk of currentChunks) {
                    combinedAudio.set(chunk, position);
                    position += chunk.length;
                }
                
                // Create WAV file from the audio buffer
                const wavBlob = createWavFromAudioBuffer(combinedAudio, audioContext.sampleRate);
                
                console.log(`Created WAV blob: ${wavBlob.size} bytes`);
                
                // Create form data
                const formData = new FormData();
                formData.append('audio', wavBlob, 'voice_segment.wav');
                formData.append('session_id', socket.id); // Use Socket.IO id
                
                // Send to parallel pipeline endpoint
                const response = await fetch('/process_audio_parallel', {
                    method: 'POST',
                    body: formData
                });
                
                const data = await response.json();
                if (!data.success) {
                    console.error('Error starting parallel processing:', data.error);
                    logMessage("Voice processing error: " + (data.error || "Unknown error"));
                } else {
                    console.log('Parallel voice processing started:', data);
                    logMessage("Processing your speech...");
                }
            } catch (error) {
                console.error("Error processing voice segment:", error);
                logMessage("Error processing speech: " + error.message);
            } finally {
                isProcessingTranscript = false;
            }
        }

        // Helper function to write string to DataView
        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }

        

        // Helper function to write string to DataView
        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }

        async function stopRecording() {
            console.log("stopRecording() called");
            
            // If in continuous mode, process any pending audio then stop
            if (continuousRecording) {
                console.log("Stopping continuous recording and processing final audio");
                
                // Process any remaining audio if we have detected voice
                if (voiceDetected && rawAudioChunks.length > 0) {
                    console.log("Processing final audio segment before stopping");
                    logMessage("Processing final speech segment...");
                    
                    try {
                        // Make processVoiceSegment return a promise we can await
                        // You may need to modify processVoiceSegment to return a Promise
                        isProcessingTranscript = true;
                        const currentChunks = [...rawAudioChunks]; // Copy the chunks for processing
                        
                        // Combine all audio chunks into one Float32Array
                        let totalLength = 0;
                        for (const chunk of currentChunks) {
                            totalLength += chunk.length;
                        }
                        
                        const combinedAudio = new Float32Array(totalLength);
                        let position = 0;
                        
                        for (const chunk of currentChunks) {
                            combinedAudio.set(chunk, position);
                            position += chunk.length;
                        }
                        
                        // Create WAV file from the audio buffer
                        const wavBlob = createWavFromAudioBuffer(combinedAudio, audioContext.sampleRate);
                        
                        // Send for transcription and wait for it to complete
                        await sendAudioForTranscription(wavBlob);
                    } catch (error) {
                        console.error("Error processing final audio segment:", error);
                        logMessage("Error processing final speech: " + error.message);
                    }
                }
                
                // Reset continuous recording mode
                continuousRecording = false;
                
                // Reset button appearance
                const recordingContainer = document.querySelector('.button-split-container:nth-of-type(2)');
                const startButton = recordingContainer.querySelector('.split-button:first-child');
                startButton.style.backgroundColor = '#1a5276'; // Default blue
                startButton.textContent = 'START AUTO RECORDING';
                
                // Stop active recording
                if (mediaRecorder && mediaRecorder.state === 'recording') {
                    stopClientRecording(false);
                }
                
                // Clean up other resources
                if (silenceDetectionTimeout) {
                    clearTimeout(silenceDetectionTimeout);
                    silenceDetectionTimeout = null;
                }
                
                // Close any open audio contexts
                if (audioContext && audioContext.state !== 'closed') {
                    try {
                        await audioContext.close();
                    } catch (err) {
                        console.error("Error closing audio context:", err);
                    }
                }
                
                // Stop the media stream tracks
                if (voiceActivationStream) {
                    voiceActivationStream.getTracks().forEach(track => track.stop());
                    voiceActivationStream = null;
                }
                
                // Reset flags
                voiceActivationEnabled = false;
                isProcessingTranscript = false;
                
                logMessage("Continuous voice-activated recording stopped.");
            } else {
                // Normal stop recording behavior
                if (mediaRecorder && mediaRecorder.state === 'recording') {
                    stopClientRecording(false);
                } else {
                    // Fall back to server-side stop
                    try {
                        const response = await fetch('/stop_recording', { method: 'POST' });
                        const data = await response.json();
                        
                        if (data.success && data.transcript) {
                            document.getElementById('textInput').value = data.transcript;
                            logMessage("Recording stopped. Transcript: " + data.transcript);
                        } else {
                            logMessage("Recording stopped with no valid transcript.");
                        }
                    } catch (error) {
                        console.error("Error stopping recording:", error);
                        logMessage("Error stopping recording: " + error.message);
                    }
                }
            }
        }

        // New utility function to ensure voice activation is running
        function ensureVoiceActivation() {
            if (!continuousRecording) {
                // Start continuous recording if not already running
                startRecording();
            } else if (voiceActivationEnabled) {
                // Already running, just log status
                logMessage("Voice activation already running and listening...");
            } else {
                // Restart if enabled but not actively listening
                startVoiceActivatedRecording();
                logMessage("Restarted voice activation...");
            }
        }

        // Modified function to periodically check if recording has stopped
        async function checkRecordingStatus() {
            try {
                const response = await fetch('/check_recording_status', { method: 'POST' });
                const data = await response.json();
                
                // If there was an error, log it and stop checking
                if (data.error) {
                    console.error("Error checking recording status:", data.error);
                    clearInterval(recordingCheckInterval);
                    recordingCheckInterval = null;
                    return;
                }
                
                // If recording is no longer active
                if (!data.is_recording) {
                    console.log("Recording has stopped:", data);
                    clearInterval(recordingCheckInterval);
                    recordingCheckInterval = null;
                    
                    // If it was auto-stopped and has a transcript
                    if (data.auto_stopped && data.transcript) {
                        // Clean up transcript - remove content within parentheses
                        const cleanedTranscript = cleanupTranscript(data.transcript);
                        
                        console.log("Auto-stopped with transcript:", cleanedTranscript);
                        
                        // Update the textbox with the transcript
                        document.getElementById('textInput').value = cleanedTranscript;
                        
                        // Log the message
                        logMessage("Recording auto-stopped after silence. Transcript: " + cleanedTranscript);
                        
                        // If auto mode is enabled, send to LLM
                        const autoMode = document.getElementById('autoModeToggle').checked;
                        if (autoMode) {
                            logMessage("Auto mode: Automatically sending transcript to LLM...");
                            await sendLLM();
                        } else {
                            // NEW CODE: Automatically trigger sendLLM to populate the reply boxes
                            // but don't auto-select a response
                            logMessage("Getting LLM responses...");
                            await sendLLM();
                            if (continuousRecording) {
                                logMessage("Continuing voice-activated recording...");
                            } else {
                                const autoMode = document.getElementById('autoModeToggle').checked;
                                if (autoMode) {
                                    // Start a new recording if in auto mode
                                    startRecording();
                                }
                            }
                        }
                    } else {
                        // Either not auto-stopped or no valid transcript
                        logMessage("Recording stopped with no valid transcript.");
                        if (continuousRecording) {
                            logMessage("Continuing voice-activated recording...");
                        } else {
                            const autoMode = document.getElementById('autoModeToggle').checked;
                            if (autoMode) {
                                // Start a new recording if in auto mode
                                startRecording();
                            }
                        }
                    }
                }
            } catch (error) {
                console.error("Error checking recording status:", error);
                // Continue checking despite errors
            }
        }
        // Conversation Starters functions
        function showStarterModal() {
            console.log("showStarterModal() called");
            // Hide the starter form initially when opening the modal
            document.getElementById('starterForm').style.display = 'none';
            loadStarters();
            document.getElementById('starterModal').style.display = 'block';
        }
        
        function closeStarterModal() {
            document.getElementById('starterModal').style.display = 'none';
            // Reset form state
            resetStarterForm();
        }
        
        function resetStarterForm() {
            document.getElementById('starterTextInput').value = '';
            document.getElementById('starterForm').style.display = 'none';
            currentEditingStarter = null;
            isCreatingNewStarter = false;
        }
        
        async function loadStarters() {
            try {
                // Load conversation starters from the server
                let starters = [];
                try {
                    const response = await fetch('/get_phrases', { 
                        method: 'POST',
                        headers: {'Content-Type': 'application/json'},
                        body: JSON.stringify({ type: 'starter' })
                    });
                    const data = await response.json();
                    if (data.phrases) {
                        starters = data.phrases;
                    }
                } catch (error) {
                    console.error("Error loading conversation starters:", error);
                    // Default starters if there's an error
                    starters = [
                        "Hello, how are you doing today?",
                        "Good to see you, what would you like to talk about?",
                        "Hi there, I hope you're having a great day.",
                        "Welcome, what's on your mind today?",
                        "Hello, it's nice to meet you."
                    ];
                }
                
                // If we still have no starters, add defaults
                if (!starters || starters.length === 0) {
                    starters = [
                        "Hello, how are you doing today?",
                        "Good to see you, what would you like to talk about?",
                        "Hi there, I hope you're having a great day.",
                        "Welcome, what's on your mind today?",
                        "Hello, it's nice to meet you."
                    ];
                    // Save default starters
                    try {
                        for (const starter of starters) {
                            await fetch('/update_phrase', {
                                method: 'POST',
                                headers: {'Content-Type': 'application/json'},
                                body: JSON.stringify({ 
                                    type: 'starter',
                                    old_text: '',
                                    new_text: starter 
                                })
                            });
                        }
                    } catch (e) {
                        console.error("Error saving default starters:", e);
                    }
                }
                
                const container = document.getElementById('starterContainer');
                container.innerHTML = ''; // Clear existing starters
                
                // Add each starter as a clickable rectangle
                starters.forEach(starter => {
                    const starterElement = document.createElement('div');
                    starterElement.className = 'phrase-item';
                    starterElement.textContent = starter;
                    
                    // Execute starter on click
                    starterElement.onclick = () => executeStarter(starter);
                    
                    // Edit on right-click or context menu
                    starterElement.oncontextmenu = (e) => {
                        e.preventDefault();
                        editStarter(starter);
                        return false;
                    };
                    
                    container.appendChild(starterElement);
                });
                
                console.log(`Loaded ${starters.length} conversation starters`);
                
                // Display message if no starters
                if (starters.length === 0) {
                    const emptyMessage = document.createElement('div');
                    emptyMessage.textContent = 'No conversation starters available. Please add one below.';
                    emptyMessage.style.color = '#666';
                    emptyMessage.style.padding = '10px';
                    container.appendChild(emptyMessage);
                }
            } catch (error) {
                console.error("Error in loadStarters:", error);
                logMessage("Error loading conversation starters: " + error);
                
                // Add fallback message to container
                const container = document.getElementById('starterContainer');
                container.innerHTML = '<div style="color: #666; padding: 10px;">Error loading conversation starters. Please try again.</div>';
            }
        }
        
        // Updated executeStarter function for continuous voice-activated recording
        async function executeStarter(text) {
            console.log("Executing conversation starter:", text);
            
            try {
                // Set this text as Bernard's statement
                lastBernardStatement = text;
                
                // First close the modal
                closeStarterModal();
                
                // Update the textbox with Bernard's statement for context
                document.getElementById('textInput').value = "Bernard: " + text;
                
                // Make API call to speak text
                const response = await fetch('/speak_text', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        text: text,
                        bernard_text: text  
                    })
                });
                
                const data = await response.json();
                console.log("executeStarter response:", data);
                
                if (data.error) {
                    logMessage("Error: " + data.error);
                } else {
                    logMessage("Bernard is speaking the conversation starter. Voice activation will start after.");
                    
                    // Play audio if URL is provided, then start voice-activated recording when done
                    if (data.audio_url) {
                        playAudioWithCallback(data.audio_url, () => {
                            // Start voice-activated recording after audio finishes
                            startRecording();  // This now toggles continuous recording mode
                        });
                    } else {
                        // If no audio URL (unlikely), just start voice-activated recording
                        setTimeout(() => startRecording(), 500);
                    }
                }
            } catch (error) {
                console.error("Error in executeStarter:", error);
                logMessage("Error: " + error);
            }
        }

        function editStarter(text) {
            console.log("Editing conversation starter:", text);
            currentEditingStarter = text;
            isCreatingNewStarter = false;
            
            // Fill the form with the starter text
            document.getElementById('starterTextInput').value = text;
            
            // Show the form and update title
            document.getElementById('starterFormTitle').textContent = 'Edit Conversation Starter';
            document.getElementById('starterForm').style.display = 'block';
            
            // Focus on the text field
            document.getElementById('starterTextInput').focus();
        }
        
        function showNewStarterForm() {
            console.log("showNewStarterForm() called");
            
            // Reset and prepare form
            document.getElementById('starterTextInput').value = '';
            document.getElementById('starterFormTitle').textContent = 'Add New Conversation Starter';
            
            // Show the form
            document.getElementById('starterForm').style.display = 'block';
            
            // Set flags
            isCreatingNewStarter = true;
            currentEditingStarter = null;
            
            // Focus on the text field
            document.getElementById('starterTextInput').focus();
        }
        
        function cancelStarterEdit() {
            resetStarterForm();
        }
        
        async function saveStarterDetails() {
            console.log("saveStarterDetails() called");
            
            // Get form value
            const text = document.getElementById('starterTextInput').value.trim();
            
            if (!text) {
                alert("Conversation starter text is required");
                return;
            }
            
            try {
                // Save the starter
                const response = await fetch('/update_phrase', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        type: 'starter',
                        old_text: isCreatingNewStarter ? '' : currentEditingStarter,
                        new_text: text
                    })
                });
                
                const data = await response.json();
                console.log("saveStarterDetails response:", data);
                
                if (data.error) {
                    alert("Error saving conversation starter: " + data.error);
                } else {
                    logMessage(data.message || "Conversation starter saved");
                    
                    // Close the form and reset
                    resetStarterForm();
                    
                    // Reload the starters list
                    loadStarters();
                }
            } catch (error) {
                console.error("Error in saveStarterDetails:", error);
                alert("Error saving conversation starter: " + error);
            }
        }
        
        // Conversation Enders functions
        function showEnderModal() {
            console.log("showEnderModal() called");
            // Hide the ender form initially when opening the modal
            document.getElementById('enderForm').style.display = 'none';
            loadEnders();
            document.getElementById('enderModal').style.display = 'block';
        }
        
        function closeEnderModal() {
            document.getElementById('enderModal').style.display = 'none';
            // Reset form state
            resetEnderForm();
        }
        
        function resetEnderForm() {
            document.getElementById('enderTextInput').value = '';
            document.getElementById('enderForm').style.display = 'none';
            currentEditingEnder = null;
            isCreatingNewEnder = false;
        }
        
        async function loadEnders() {
            try {
                // Load conversation enders from the server
                let enders = [];
                try {
                    const response = await fetch('/get_phrases', { 
                        method: 'POST',
                        headers: {'Content-Type': 'application/json'},
                        body: JSON.stringify({ type: 'ender' })
                    });
                    const data = await response.json();
                    if (data.phrases) {
                        enders = data.phrases;
                    }
                } catch (error) {
                    console.error("Error loading conversation enders:", error);
                    // Default enders if there's an error
                    enders = [
                        "Thank you for the conversation. Have a great day!",
                        "It was nice talking to you. Goodbye for now.",
                        "I need to go now, but we can continue our conversation later.",
                        "Thank you for your time today. Let's talk again soon.",
                        "I've enjoyed our conversation. Until next time!"
                    ];
                }
                
                // If we still have no enders, add defaults
                if (!enders || enders.length === 0) {
                    enders = [
                        "Thank you for the conversation. Have a great day!",
                        "It was nice talking to you. Goodbye for now.",
                        "I need to go now, but we can continue our conversation later.",
                        "Thank you for your time today. Let's talk again soon.",
                        "I've enjoyed our conversation. Until next time!"
                    ];
                    // Save default enders
                    try {
                        for (const ender of enders) {
                            await fetch('/update_phrase', {
                                method: 'POST',
                                headers: {'Content-Type': 'application/json'},
                                body: JSON.stringify({ 
                                    type: 'ender',
                                    old_text: '',
                                    new_text: ender 
                                })
                            });
                        }
                    } catch (e) {
                        console.error("Error saving default enders:", e);
                    }
                }
                
                const container = document.getElementById('enderContainer');
                container.innerHTML = ''; // Clear existing enders
                
                // Add each ender as a clickable rectangle
                enders.forEach(ender => {
                    const enderElement = document.createElement('div');
                    enderElement.className = 'phrase-item';
                    enderElement.textContent = ender;
                    
                    // Execute ender on click
                    enderElement.onclick = () => executeEnder(ender);
                    
                    // Edit on right-click or context menu
                    enderElement.oncontextmenu = (e) => {
                        e.preventDefault();
                        editEnder(ender);
                        return false;
                    };
                    
                    container.appendChild(enderElement);
                });
                
                console.log(`Loaded ${enders.length} conversation enders`);
                
                // Display message if no enders
                if (enders.length === 0) {
                    const emptyMessage = document.createElement('div');
                    emptyMessage.textContent = 'No conversation enders available. Please add one below.';
                    emptyMessage.style.color = '#666';
                    emptyMessage.style.padding = '10px';
                    container.appendChild(emptyMessage);
                }
            } catch (error) {
                console.error("Error in loadEnders:", error);
                logMessage("Error loading conversation enders: " + error);
                
                // Add fallback message to container
                const container = document.getElementById('enderContainer');
                container.innerHTML = '<div style="color: #666; padding: 10px;">Error loading conversation enders. Please try again.</div>';
            }
        }
        
        async function executeEnder(text) {
            console.log("Executing conversation ender:", text);
            
            try {
                // First close the modal
                closeEnderModal();
                
                // Make API call to speak text (no recording after)
                const response = await fetch('/speak_text', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        text: text,
                        bernard_text: text 
                     })
                });
                
                const data = await response.json();
                console.log("executeEnder response:", data);
                
                if (data.error) {
                    logMessage("Error: " + data.error);
                    if (continuousRecording) {
                        logMessage("Continuing voice-activated recording...");
                    } else {
                        const autoMode = document.getElementById('autoModeToggle').checked;
                        if (autoMode) {
                            // Start a new recording if in auto mode
                            startRecording();
                        }
                    }
                } else {
                    logMessage("Bernard is speaking the conversation ender.");
                    
                    // Play audio if URL is provided (NO auto-recording after)
                    if (data.audio_url) {
                        document.getElementById('textInput').value = "Bernard: " + text;
                        playAudioWithCallback(data.audio_url, () => {
                            // Reset conversation after ending
                            lastBernardStatement = "";
                            if (continuousRecording) {
                                logMessage("Continuing voice-activated recording...");
                            } else {
                                const autoMode = document.getElementById('autoModeToggle').checked;
                                if (autoMode) {
                                    // Start a new recording if in auto mode
                                    startRecording();
                                }
                            }
                        });
                    } else {
                        // Reset conversation after ending
                        lastBernardStatement = "";
                        if (continuousRecording) {
                            logMessage("Continuing voice-activated recording...");
                        } else {
                            const autoMode = document.getElementById('autoModeToggle').checked;
                            if (autoMode) {
                                // Start a new recording if in auto mode
                                startRecording();
                            }
                        }
                    }
                }
            } catch (error) {
                console.error("Error in executeEnder:", error);
                logMessage("Error: " + error);
                if (continuousRecording) {
                    logMessage("Continuing voice-activated recording...");
                } else {
                    const autoMode = document.getElementById('autoModeToggle').checked;
                    if (autoMode) {
                        // Start a new recording if in auto mode
                        startRecording();
                    }
                }
            }
        }
        
        function editEnder(text) {
            console.log("Editing conversation ender:", text);
            currentEditingEnder = text;
            isCreatingNewEnder = false;
            
            // Fill the form with the ender text
            document.getElementById('enderTextInput').value = text;
            
            // Show the form and update title
            document.getElementById('enderFormTitle').textContent = 'Edit Conversation Ender';
            document.getElementById('enderForm').style.display = 'block';
            
            // Focus on the text field
            document.getElementById('enderTextInput').focus();
        }
        
        function showNewEnderForm() {
            console.log("showNewEnderForm() called");
            
            // Reset and prepare form
            document.getElementById('enderTextInput').value = '';
            document.getElementById('enderFormTitle').textContent = 'Add New Conversation Ender';
            
            // Show the form
            document.getElementById('enderForm').style.display = 'block';
            
            // Set flags
            isCreatingNewEnder = true;
            currentEditingEnder = null;
            
            // Focus on the text field
            document.getElementById('enderTextInput').focus();
        }
        
        function cancelEnderEdit() {
            resetEnderForm();
        }
        
        async function saveEnderDetails() {
            console.log("saveEnderDetails() called");
            
            // Get form value
            const text = document.getElementById('enderTextInput').value.trim();
            
            if (!text) {
                alert("Conversation ender text is required");
                return;
            }
            
            try {
                // Save the ender
                const response = await fetch('/update_phrase', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        type: 'ender',
                        old_text: isCreatingNewEnder ? '' : currentEditingEnder,
                        new_text: text
                    })
                });
                
                const data = await response.json();
                console.log("saveEnderDetails response:", data);
                
                if (data.error) {
                    alert("Error saving conversation ender: " + data.error);
                } else {
                    logMessage(data.message || "Conversation ender saved");
                    
                    // Close the form and reset
                    resetEnderForm();
                    
                    // Reload the enders list
                    loadEnders();
                }
            } catch (error) {
                console.error("Error in saveEnderDetails:", error);
                alert("Error saving conversation ender: " + error);
            }
        }
        
        // Say again function that can be called from the starter modal
        async function sayAgainFromModal() {
            console.log("sayAgainFromModal() called");
            
            // Close the modal
            closeStarterModal();
            
            // Call the existing say again function
            await sayAgain();
        }
        
        // Updated startConversationWithRecording for continuous voice-activated recording
        async function startConversationWithRecording() {
            console.log("startConversationWithRecording() called");
            const text = document.getElementById('textInput').value;
            if (!text.trim()) {
                logMessage("Please enter text before starting conversation");
                return;
            }
            // Save Bernard's statement for context
            lastBernardStatement = text;

            try {
                const response = await fetch('/speak_text', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        text: text,
                        bernard_text: text  // Add this line to indicate this is Bernard's text
                    })
                });
                const data = await response.json();
                console.log("startConversationWithRecording response:", data);
                if (data.error) {
                    logMessage("Error: " + data.error);
                } else {
                    logMessage("Bernard is speaking the text. Voice activation will start after.");
                    
                    // Play audio if URL is provided, then start voice-activated recording when done
                    if (data.audio_url) {
                        playAudioWithCallback(data.audio_url, () => {
                            // If we're not already in continuous recording mode, start it
                            if (!continuousRecording) {
                                startRecording();  // This toggles continuous recording mode
                            } else {
                                // If we're already in continuous mode, just make sure we're listening
                                logMessage("Continuing voice-activated recording...");
                            }
                        });
                    }
                }
            } catch (error) {
                console.error("Error in startConversationWithRecording:", error);
                logMessage("Error: " + error);
            }
        }
        

        // Modified sendLLM function to integrate better with voice-activated recording
        async function sendLLM(transcript = null) {
            console.log("sendLLM() called");
            
            // If no transcript provided, get it from the textbox
            let prompt = transcript || document.getElementById('textInput').value;
            
            if (!prompt.trim()) {
                logMessage("Please enter text before sending to LLM");
                return;
            }
            
            logMessage("Sending to LLM...");
            const startTime = performance.now(); 
            
            // Get toggle states
            const useContext = document.getElementById('contextToggle').checked;
            const autoMode = document.getElementById('autoModeToggle').checked;
            const useInternet = document.getElementById('internetToggle').checked;
            
            if (useInternet) {
                logMessage("Internet access enabled. Retrieving information...");
            }
            
            try {
                // Extract just the user's message if we have Bernard/User format
                let userTranscript = prompt;
                if (prompt.includes("Bernard:") && prompt.includes("User:")) {
                    const parts = prompt.split("User:");
                    if (parts.length > 1) {
                        userTranscript = parts[1].trim();
                    }
                }
                
                // Build limited conversation context for automatic processing
                let contextMessages = [];
                if (transcript) {  // If it's from voice transcript
                    const recentExchanges = currentConversation.exchanges.slice(-3);
                    for (const exchange of recentExchanges) {
                        if (exchange.bernard) {
                            contextMessages.push({ role: "assistant", content: exchange.bernard });
                        }
                        if (exchange.user) {
                            contextMessages.push({ role: "user", content: exchange.user });
                        }
                    }
                }
                
                const response = await fetch('/send_llm', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        prompt: userTranscript,
                        conversationContext: transcript ? contextMessages : [], // Only send context for voice transcripts
                        useContext,
                        autoMode,
                        useInternet
                    })
                });
                
                const data = await response.json();
                if (data.responseTime) {
                    logMessage(`LLM responses received in ${data.responseTime} seconds`);
                }
                console.log("sendLLM response:", data);
                
                if (data.error) {
                    logMessage("Error: " + data.error);
                    return;
                }
                
                // Reset conversation context after sending to LLM
                // Only if not in auto mode (since auto mode continues the conversation)
                if (!autoMode) {
                    lastBernardStatement = "";
                }
                
                // Display internet information if available
                if (useInternet && data.internetInfo) {
                    logMessage("Internet information: " + data.internetInfo);
                }
                
                if (autoMode) {
                    // In auto mode, just speak the first response directly
                    const chosenResponse = data.completions[0] || "";
                    logMessage("Auto mode: Speaking response directly.");
                    
                    // Make API call to speak text
                    const speakResponse = await fetch('/speak_text', {
                        method: 'POST',
                        headers: {'Content-Type': 'application/json'},
                        body: JSON.stringify({ 
                            text: chosenResponse,
                            user_text: prompt 
                        })
                    });
                    
                    const speakData = await speakResponse.json();
                    if (speakData.error) {
                        logMessage("Error in auto-speak: " + speakData.error);
                    } else {
                        // Store Bernard's statement for context
                        lastBernardStatement = chosenResponse;
                        
                        // Start voice-activated recording after speech
                        if (speakData.audio_url) {
                            playAudioWithCallback(speakData.audio_url, () => {
                                console.log('[DEBUG] TTS finished, triggering recordAfterAudio');
                                recordAfterAudio();
                            });
                        }
                    }
                } else {
                    // Regular mode: show all options
                    // Clear all return boxes first
                    document.getElementById('return1').value = "";
                    document.getElementById('return2').value = "";
                    document.getElementById('return3').value = "";
                    document.getElementById('return4').value = "";
                    
                    // Fill in the responses
                    document.getElementById('return1').value = data.completions[0] || "";
                    document.getElementById('return2').value = data.completions[1] || "";
                    document.getElementById('return3').value = data.completions[2] || "";
                    document.getElementById('return4').value = data.completions[3] || "";
                    
                    logMessage("LLM responses received. Click on any blue response box to select it.");
                    
                    // If we're in continuous recording mode, keep listening
                    if (continuousRecording && voiceActivationEnabled) {
                        logMessage("Continuing voice-activated recording...");
                    }
                }
            } catch (error) {
                console.error("Error in sendLLM:", error);
                logMessage("Error: " + error);
            }
        }

        // Buffer LLM responses
        function queueLLMRequest(transcript) {
            const pendingRequest = {
                id: Date.now(),
                transcript: transcript,
                timestamp: new Date()
            };
            
            currentConversation.pendingResponses.push(pendingRequest);
            
            // Process the request with a debounce to handle rapid inputs
            if (!currentConversation.processingTimeout) {
                currentConversation.processingTimeout = setTimeout(() => {
                    processPendingLLMRequests();
                }, 1000); // 1 second debounce
            }
        }

        async function processPendingLLMRequests() {
            clearTimeout(currentConversation.processingTimeout);
            currentConversation.processingTimeout = null;
            
            if (currentConversation.pendingResponses.length === 0) return;
            
            // Process the most recent request only
            const latestRequest = currentConversation.pendingResponses.pop();
            currentConversation.pendingResponses = []; // Clear the queue
            
            // Send to LLM with only the current exchange
            await sendLLM(latestRequest.transcript);
        }
        
        // Updated selectReply function for continuous voice-activated recording
        // Modified selectReply function to handle TTS in parallel
        async function selectReply(areaId) {
            console.log("selectReply() called for area:", areaId);
            const chosen_text = document.getElementById(areaId).value;
            if (!chosen_text.trim()) {
                logMessage("No response to select in this box");
                return;
            }
            
            try {
                // Store Bernard's statement for context immediately
                lastBernardStatement = chosen_text;
                
                // Show immediate feedback to user
                logMessage("Speaking selected reply...");

                // Update the textbox to show the conversation
                const originalInput = document.getElementById('textInput').value;
                // If the text already has a Bernard: prefix, replace it
                if (originalInput.includes("Bernard:") && originalInput.includes("User:")) {
                    // Extract the user part and create a new display
                    const userPart = originalInput.split("User:")[1].trim();
                    // SWAP ORDER: User first, then Bernard
                    document.getElementById('textInput').value = `User: ${userPart}\n\nBernard: ${chosen_text}`;
                } else if (originalInput.trim()) {
                    // If there's text but no prefix format, use it as user text
                    // SWAP ORDER: User first, then Bernard
                    document.getElementById('textInput').value = `User: ${originalInput.trim()}\n\nBernard: ${chosen_text}`;
                }
                // Check if this is after an enhance prompt operation
                const isEnhancedPrompt = 
                    (document.getElementById('return1').value.trim() !== "" || 
                    document.getElementById('return2').value.trim() !== "" || 
                    document.getElementById('return3').value.trim() !== "" || 
                    document.getElementById('return4').value.trim() !== "") &&
                    originalInput.trim().toLowerCase().startsWith("bernard:");
                
                // Get the Socket.IO session ID
                const sessionId = socket.id;
                
                // For enhanced prompts, use the old API
                if (isEnhancedPrompt) {
                    // Use the old API for enhanced prompts
                    const speakPromise = fetch('/speak_text', {
                        method: 'POST',
                        headers: {'Content-Type': 'application/json'},
                        body: JSON.stringify({ 
                            text: chosen_text,
                            user_text: isEnhancedPrompt ? "" : stripPrefixes(originalInput),
                            bernard_text: "",
                            is_enhanced_prompt: isEnhancedPrompt
                        })
                    });
                    
                    speakPromise.then(response => response.json())
                        .then(data => {
                            console.log("selectReply TTS response:", data);
                            if (data.error) {
                                logMessage("Error: " + data.error);
                            } else if (data.audio_url) {
                                // Play the audio when it's ready
                                playAudioWithCallback(data.audio_url, () => {
                                    console.log('[DEBUG] TTS finished, triggering recordAfterAudio');
                                    recordAfterAudio();
                                });
                            }
                        })
                        .catch(error => {
                            console.error("Error in TTS processing:", error);
                            logMessage("Error in text-to-speech: " + error);
                        });
                }
                // For normal completions, use the new select_completion API
                else {
                    // Call the new API endpoint to select a completion
                    const response = await fetch('/select_completion', {
                        method: 'POST',
                        headers: {'Content-Type': 'application/json'},
                        body: JSON.stringify({ 
                            session_id: sessionId,
                            completion_text: chosen_text,
                            // We also calculate the index of the selected completion
                            completion_index: areaId === 'return1' ? 0 : 
                                            areaId === 'return2' ? 1 : 
                                            areaId === 'return3' ? 2 : 
                                            areaId === 'return4' ? 3 : -1
                        })
                    });
                    
                    const data = await response.json();
                    
                    if (data.error) {
                        logMessage("Error: " + data.error);
                    } else {
                        // Update the UI to show that processing has started
                        logMessage("Processing selected completion...");
                        
                        // The rest will be handled by Socket.IO events
                        // Note: We don't await the full TTS process - Socket.IO will notify
                        // us as chunks become available
                        
                        // Don't start continuous recording yet - that will happen
                        // when TTS playback is complete via the Socket.IO events
                    }
                }
            } catch (error) {
                console.error("Error in selectReply:", error);
                logMessage("Error: " + error);
            }
        }
        // Function to show settings modal
        function showSettings() {
            console.log("showSettings() called");
            loadAllSettings();
            document.getElementById('settingsModal').style.display = 'block';
        }

        // Function to close settings modal
        function closeSettingsModal() {
            document.getElementById('settingsModal').style.display = 'none';
        }

        // Function to handle tab switching
        function openSettingsTab(evt, tabName) {
            // Hide all tab content
            const tabcontent = document.getElementsByClassName("settings-tab-content");
            for (let i = 0; i < tabcontent.length; i++) {
                tabcontent[i].style.display = "none";
            }

            // Remove "active" class from all tab buttons
            const tablinks = document.getElementsByClassName("settings-tab-btn");
            for (let i = 0; i < tablinks.length; i++) {
                tablinks[i].className = tablinks[i].className.replace(" active", "");
            }

            // Show the specific tab content and add "active" class to the button
            document.getElementById(tabName).style.display = "block";
            evt.currentTarget.className += " active";
        }

        // Function to load all settings from the server
        async function loadAllSettings() {
            try {
                // Make API call to get settings
                const response = await fetch('/get_settings', { method: 'GET' });
                const data = await response.json();
                console.log("Settings loaded:", data);
                
                // Store the settings data globally
                currentSettingsData = data;
                
                // Load settings into each tab
                loadVoiceSettings(data.voice || {});
                loadLLMSettings(data.llm || {});
                loadRecorderSettings(data.recorder || {});
                loadSystemSettings(data.system || {});
                
                logMessage("Settings loaded successfully");
            } catch (error) {
                console.error("Error loading settings:", error);
                logMessage("Error loading settings: " + error);
                
                // Load default settings
                loadDefaultSettings();
            }
        }

        // Function to load default settings if server call fails
        function loadDefaultSettings() {
            console.log("Loading default settings");
            
            // Default voice settings
            loadVoiceSettings({
                apiKey: "",
                voiceId: "eSAnsg5EvoUbefcXwEKT",
                model: "eleven_multilingual_v2",
                outputFormat: "mp3_44100_192",
                seed: 8675309,
                voiceSettings: {
                    stability: 0.5,
                    similarity_boost: 0.75,
                    speed: 0.9,
                    style: 0.3
                }
            });
            
            // Default LLM settings
            loadLLMSettings({
                openai: {
                    apiKey: "",
                    model: "gpt-4",
                    temperature: 0.9,
                    maxTokens: 150,
                    completionsCount: 4
                },
                perplexity: {
                    apiKey: "",
                    model: "sonar"
                }
            });
            
            // Default recorder settings
            loadRecorderSettings({
                silenceThreshold: 500,
                silenceDuration: 5.0,
                minRecordingDuration: 1.0,
                maxRecordingDuration: 60.0,
                useNoiseReduction: true
            });
            
            // Default system settings
            loadSystemSettings({
                systemPrompt: `You are the persona Dita, a friendly Digital Twin Assistant. \n\n
                 4.Communication style:\n   
                 - Dita Uses eye-tracking technology to communicate\n   
                 - respond Friendly, sometimes sarcastic and precise with humor\n   
                 - Use an accessible, informal tone\n   
                 - May swear from time to time \n   
                 - Give direct, factual answers\n   
                 - Avoid excessive formality\n\n
                 5. Prompt and chat completion instructions\n  
                 - Please keep your responses to a maximum of one sentence\n  
                 - For each completion in a set of multiple completions, adopt a distinctly different tone, perspective, or approach\n  
                 - Make sure each response offers unique information or a unique angle on the same information\n  
                 - When answering binary questions (e.g., yes/no), Ensure that your answers include both 'yes' and 'no' options, maintaining diversity in the responses.`,
                fileLocations: {
                    kbDir: "data/kb",
                    vectorStoreDir: "data/vector_store",
                    chatHistoryFile: "data/chat_history.json",
                    phrasesFile: "data/phrases.json"
                },
                language: {
                    defaultLanguage: "auto",
                    useLanguageDetection: true
                },
                debugMode: false
            });
        }

        // Function to load voice settings into UI
        function loadVoiceSettings(settings) {
            // Set voice API fields
            document.getElementById('elevenLabsApiKey').value = settings.apiKey || "";
            document.getElementById('voiceId').value = settings.voiceId || "eSAnsg5EvoUbefcXwEKT";
            document.getElementById('ttsModel').value = settings.model || "eleven_multilingual_v2";
            document.getElementById('outputFormat').value = settings.outputFormat || "mp3_44100_192";
            document.getElementById('voiceSeed').value = settings.seed || 8675309;
            
            // Set voice parameters
            const voiceSettings = settings.voiceSettings || {
                stability: 0.5,
                similarity_boost: 0.75,
                speed: 0.9,
                style: 0.3
            };
            
            document.getElementById('voiceStability').value = voiceSettings.stability || 0.5;
            document.getElementById('voiceStabilityValue').textContent = voiceSettings.stability || 0.5;
            
            document.getElementById('voiceSimilarity').value = voiceSettings.similarity_boost || 0.75;
            document.getElementById('voiceSimilarityValue').textContent = voiceSettings.similarity_boost || 0.75;
            
            document.getElementById('voiceSpeed').value = voiceSettings.speed || 0.9;
            document.getElementById('voiceSpeedValue').textContent = voiceSettings.speed || 0.9;
            
            document.getElementById('voiceStyle').value = voiceSettings.style || 0.3;
            document.getElementById('voiceStyleValue').textContent = voiceSettings.style || 0.3;
            
            // Set up range input event listeners to update the displayed values
            setupRangeListeners();
        }

        // Function to load LLM settings into UI
        function loadLLMSettings(settings) {
            // Set provider
            const provider = settings.provider || "openai";
            document.getElementById('llmProvider').value = provider;
            
            // Show the correct model section
            if (provider === 'openai') {
                document.getElementById('openaiModels').style.display = 'block';
                document.getElementById('groqModels').style.display = 'none';
            } else if (provider === 'groq') {
                document.getElementById('openaiModels').style.display = 'none';
                document.getElementById('groqModels').style.display = 'block';
            }
            
            // Set OpenAI fields
            const openai = settings.openai || {};
            document.getElementById('openaiApiKey').value = openai.apiKey || "";
            document.getElementById('openaiModel').value = openai.model || "gpt-4";
            
            // Set Groq fields
            const groq = settings.groq || {};
            document.getElementById('groqApiKey').value = groq.apiKey || "";
            document.getElementById('groqModel').value = groq.model || "llama-3.3-70b-specdec";
            
            // Set common fields
            document.getElementById('llmTemperature').value = settings.temperature || 0.9;
            document.getElementById('llmTemperatureValue').textContent = settings.temperature || 0.9;
            document.getElementById('maxTokens').value = settings.maxTokens || 150;
            document.getElementById('completionsCount').value = settings.completionsCount || 4;
            
            // Set Perplexity fields
            const perplexity = settings.perplexity || {};
            document.getElementById('perplexityApiKey').value = perplexity.apiKey || "";
            document.getElementById('perplexityModel').value = perplexity.model || "sonar";
        }

        // Function to load recorder settings into UI
        function loadRecorderSettings(settings) {
            document.getElementById('silenceThreshold').value = settings.silenceThreshold || 500;
            document.getElementById('silenceDuration').value = settings.silenceDuration || 5.0;
            document.getElementById('minRecordingDuration').value = settings.minRecordingDuration || 1.0;
            document.getElementById('maxRecordingDuration').value = settings.maxRecordingDuration || 60.0;
            document.getElementById('useNoiseReduction').checked = settings.useNoiseReduction !== false;
        }

        // Function to load system settings into UI
        function loadSystemSettings(settings) {
            // Set system prompt
            document.getElementById('systemPrompt').value = settings.systemPrompt || "";
            
            // Set file locations
            const fileLocations = settings.fileLocations || {};
            document.getElementById('kbDir').value = fileLocations.kbDir || "data/kb";
            document.getElementById('vectorStoreDir').value = fileLocations.vectorStoreDir || "data/vector_store";
            document.getElementById('chatHistoryFile').value = fileLocations.chatHistoryFile || "data/chat_history.json";
            document.getElementById('phrasesFile').value = fileLocations.phrasesFile || "data/phrases.json";
            
            // Set language settings
            const language = settings.language || {};
            document.getElementById('defaultLanguage').value = language.defaultLanguage || "auto";
            document.getElementById('useLanguageDetection').checked = language.useLanguageDetection !== false;
            
            // Set debug mode
            document.getElementById('debugMode').checked = settings.debugMode === true;
        }

        // Function to set up event listeners for range inputs
        function setupRangeListeners() {
            // Voice settings sliders
            document.getElementById('voiceStability').addEventListener('input', function() {
                document.getElementById('voiceStabilityValue').textContent = this.value;
            });
            
            document.getElementById('voiceSimilarity').addEventListener('input', function() {
                document.getElementById('voiceSimilarityValue').textContent = this.value;
            });
            
            document.getElementById('voiceSpeed').addEventListener('input', function() {
                document.getElementById('voiceSpeedValue').textContent = this.value;
            });
            
            document.getElementById('voiceStyle').addEventListener('input', function() {
                document.getElementById('voiceStyleValue').textContent = this.value;
            });
            
            // LLM temperature slider
            document.getElementById('llmTemperature').addEventListener('input', function() {
                document.getElementById('llmTemperatureValue').textContent = this.value;
            });
        }

        // Function to save voice settings
        async function saveVoiceSettings() {
            try {
                const voiceSettings = {
                    apiKey: document.getElementById('elevenLabsApiKey').value,
                    voiceId: document.getElementById('voiceId').value,
                    model: document.getElementById('ttsModel').value,
                    outputFormat: document.getElementById('outputFormat').value,
                    seed: parseInt(document.getElementById('voiceSeed').value),
                    voiceSettings: {
                        stability: parseFloat(document.getElementById('voiceStability').value),
                        similarity_boost: parseFloat(document.getElementById('voiceSimilarity').value),
                        speed: parseFloat(document.getElementById('voiceSpeed').value),
                        style: parseFloat(document.getElementById('voiceStyle').value)
                    }
                };
                
                // Make API call to save settings
                const response = await fetch('/save_settings', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        category: 'voice',
                        settings: voiceSettings
                    })
                });
                
                const data = await response.json();
                console.log("Save voice settings response:", data);
                
                if (data.success) {
                    logMessage("Voice settings saved successfully");
                } else {
                    logMessage("Error saving voice settings: " + (data.message || "Unknown error"));
                }
            } catch (error) {
                console.error("Error saving voice settings:", error);
                logMessage("Error saving voice settings: " + error);
            }
        }

        // Function to test voice settings
        async function testVoiceSettings() {
            try {
                // Create a test message
                const testMessage = "Hi, this is your ElevenLabs voice. Testing voice parameters for Bernard.";
                
                // Get current voice settings
                const voiceSettings = {
                    apiKey: document.getElementById('elevenLabsApiKey').value,
                    voiceId: document.getElementById('voiceId').value,
                    model: document.getElementById('ttsModel').value,
                    outputFormat: document.getElementById('outputFormat').value,
                    seed: parseInt(document.getElementById('voiceSeed').value),
                    voiceSettings: {
                        stability: parseFloat(document.getElementById('voiceStability').value),
                        similarity_boost: parseFloat(document.getElementById('voiceSimilarity').value),
                        speed: parseFloat(document.getElementById('voiceSpeed').value),
                        style: parseFloat(document.getElementById('voiceStyle').value)
                    }
                };
                
                // Show user feedback immediately
                logMessage("Testing voice... Please wait...");
                
                // Instead of using a custom route, let's use the existing speak_text route which we know works
                const response = await fetch('/speak_text', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        text: testMessage,
                        is_enhanced_prompt: true // Flag to prevent recording in chat history
                    })
                });
                
                const data = await response.json();
                console.log("Test voice response:", data);
                
                if (data.error) {
                    logMessage("Error testing voice: " + data.error);
                } else if (data.audio_url) {
                    logMessage("Playing test voice: \"Hi, this is your ElevenLabs voice.\"");
                    
                    // Play the test audio (set volume to audible level)
                    const audioPlayer = document.getElementById('audioPlayer');
                    audioPlayer.volume = 1.0;
                    audioPlayer.src = data.audio_url;
                    audioPlayer.play();
                    
                    // Also save the settings since they worked
                    await saveVoiceSettings();
                }
            } catch (error) {
                console.error("Error testing voice:", error);
                logMessage("Error testing voice: " + error);
            }
        }

        // Function to save LLM settings
        async function saveLLMSettings() {
            try {
                const provider = document.getElementById('llmProvider').value;
                
                const llmSettings = {
                    provider: provider,
                    openai: {
                        apiKey: document.getElementById('openaiApiKey').value,
                        model: document.getElementById('openaiModel').value,
                    },
                    groq: {
                        apiKey: document.getElementById('groqApiKey').value,
                        model: document.getElementById('groqModel').value,
                    },
                    temperature: parseFloat(document.getElementById('llmTemperature').value),
                    maxTokens: parseInt(document.getElementById('maxTokens').value),
                    completionsCount: parseInt(document.getElementById('completionsCount').value),
                    perplexity: {
                        apiKey: document.getElementById('perplexityApiKey').value,
                        model: document.getElementById('perplexityModel').value
                    }
                };
                
                // Make API call to save settings
                const response = await fetch('/save_settings', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        category: 'llm',
                        settings: llmSettings
                    })
                });
                
                const data = await response.json();
                console.log("Save LLM settings response:", data);
                
                if (data.success) {
                    logMessage("LLM settings saved successfully");
                } else {
                    logMessage("Error saving LLM settings: " + (data.message || "Unknown error"));
                }
            } catch (error) {
                console.error("Error saving LLM settings:", error);
                logMessage("Error saving LLM settings: " + error);
            }
        }

        // Function to test LLM settings
        async function testLLMSettings() {
            try {
                // Create a test message
                const testMessage = "This is a test of the LLM settings. What's your name and how are you feeling today?";
                
                // Get current LLM settings
                const llmSettings = {
                    openai: {
                        apiKey: document.getElementById('openaiApiKey').value,
                        model: document.getElementById('llmModel').value,
                        temperature: parseFloat(document.getElementById('llmTemperature').value),
                        maxTokens: parseInt(document.getElementById('maxTokens').value),
                        completionsCount: 1 // Just get 1 for testing
                    }
                };
                
                // Make API call to test LLM
                const response = await fetch('/test_llm', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        prompt: testMessage,
                        settings: llmSettings
                    })
                });
                
                const data = await response.json();
                console.log("Test LLM response:", data);
                
                if (data.error) {
                    logMessage("Error testing LLM: " + data.error);
                } else if (data.completion) {
                    logMessage("LLM Test Result: " + data.completion);
                }
            } catch (error) {
                console.error("Error testing LLM:", error);
                logMessage("Error testing LLM: " + error);
            }
        }

        // Function to save recorder settings
        async function saveRecorderSettings() {
            try {
                const recorderSettings = {
                    silenceThreshold: parseInt(document.getElementById('silenceThreshold').value),
                    silenceDuration: parseFloat(document.getElementById('silenceDuration').value),
                    minRecordingDuration: parseFloat(document.getElementById('minRecordingDuration').value),
                    maxRecordingDuration: parseFloat(document.getElementById('maxRecordingDuration').value),
                    useNoiseReduction: document.getElementById('useNoiseReduction').checked
                };
                
                // Validate inputs
                if (recorderSettings.silenceThreshold < 0 || recorderSettings.silenceThreshold > 2000) {
                    logMessage("Error: Silence threshold must be between 0 and 2000");
                    return;
                }
                
                if (recorderSettings.silenceDuration < 1 || recorderSettings.silenceDuration > 10) {
                    logMessage("Error: Silence duration must be between 1 and 10 seconds");
                    return;
                }
                
                if (recorderSettings.minRecordingDuration < 0.5 || recorderSettings.minRecordingDuration > 5) {
                    logMessage("Error: Minimum recording duration must be between 0.5 and 5 seconds");
                    return;
                }
                
                if (recorderSettings.maxRecordingDuration < 10 || recorderSettings.maxRecordingDuration > 300) {
                    logMessage("Error: Maximum recording duration must be between 10 and 300 seconds");
                    return;
                }
                
                // Make API call to save settings
                const response = await fetch('/save_settings', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        category: 'recorder',
                        settings: recorderSettings
                    })
                });
                
                const data = await response.json();
                console.log("Save recorder settings response:", data);
                
                if (data.success) {
                    logMessage("Recorder settings saved successfully");
                } else {
                    logMessage("Error saving recorder settings: " + (data.message || "Unknown error"));
                }
            } catch (error) {
                console.error("Error saving recorder settings:", error);
                logMessage("Error saving recorder settings: " + error);
            }
        }

        // Function to test recorder settings
        async function testRecorderSettings() {
            try {
                // Get current recorder settings
                const recorderSettings = {
                    silenceThreshold: parseInt(document.getElementById('silenceThreshold').value),
                    silenceDuration: parseFloat(document.getElementById('silenceDuration').value),
                    minRecordingDuration: parseFloat(document.getElementById('minRecordingDuration').value),
                    maxRecordingDuration: parseFloat(document.getElementById('maxRecordingDuration').value),
                    useNoiseReduction: document.getElementById('useNoiseReduction').checked
                };
                
                // Make API call to test recorder
                const response = await fetch('/test_recorder', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ settings: recorderSettings })
                });
                
                const data = await response.json();
                console.log("Test recorder response:", data);
                
                if (data.error) {
                    logMessage("Error testing recorder: " + data.error);
                } else {
                    logMessage("Recording started with test settings. Speak now and wait for silence detection...");
                    
                    // Start polling for recording status
                    checkRecordingStatusForTest();
                }
            } catch (error) {
                console.error("Error testing recorder:", error);
                logMessage("Error testing recorder: " + error);
            }
        }

        // Function to check recording status during test
        async function checkRecordingStatusForTest() {
            try {
                const response = await fetch('/check_recording_status', { method: 'POST' });
                const data = await response.json();
                
                // If there was an error, log it and stop checking
                if (data.error) {
                    console.error("Error checking recording status:", data.error);
                    logMessage("Error checking recording status: " + data.error);
                    return;
                }
                
                // If recording is no longer active
                if (!data.is_recording) {
                    console.log("Test recording has stopped:", data);
                    
                    // If it was auto-stopped and has a transcript
                    if (data.auto_stopped && data.transcript) {
                        logMessage("Test recording completed. Settings worked correctly! Transcript: " + data.transcript);
                    } else if (data.auto_stopped) {
                        logMessage("Test recording auto-stopped but no transcript was obtained.");
                    } else {
                        logMessage("Test recording stopped manually.");
                    }
                    return;
                }
                
                // Continue polling if still recording
                setTimeout(checkRecordingStatusForTest, 1000);
            } catch (error) {
                console.error("Error checking test recording status:", error);
                logMessage("Error checking test recording status: " + error);
            }
        }

        // Function to save system settings
        async function saveSystemSettings() {
            try {
                const systemSettings = {
                    systemPrompt: document.getElementById('systemPrompt').value,
                    fileLocations: {
                        kbDir: document.getElementById('kbDir').value,
                        vectorStoreDir: document.getElementById('vectorStoreDir').value,
                        chatHistoryFile: document.getElementById('chatHistoryFile').value,
                        phrasesFile: document.getElementById('phrasesFile').value
                    },
                    language: {
                        defaultLanguage: document.getElementById('defaultLanguage').value,
                        useLanguageDetection: document.getElementById('useLanguageDetection').checked
                    },
                    debugMode: document.getElementById('debugMode').checked
                };
                
                // Make API call to save settings
                const response = await fetch('/save_settings', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        category: 'system',
                        settings: systemSettings
                    })
                });
                
                const data = await response.json();
                console.log("Save system settings response:", data);
                
                if (data.success) {
                    logMessage("System settings saved successfully");
                } else {
                    logMessage("Error saving system settings: " + (data.message || "Unknown error"));
                }
            } catch (error) {
                console.error("Error saving system settings:", error);
                logMessage("Error saving system settings: " + error);
            }
        }

        // Function to reset system settings to defaults
        function resetSystemSettings() {
            if (confirm("Are you sure you want to reset all system settings to defaults?")) {
                loadSystemSettings({
                    systemPrompt: `You are the persona Dita, a friendly Digital Twin Assistant. \n\n
                                    4.Communication style:\n   
                                    - Dita Uses eye-tracking technology to communicate\n   
                                    - respond Friendly, sometimes sarcastic and precise with humor\n   
                                    - Use an accessible, informal tone\n   
                                    - May swear from time to time \n   
                                    - Give direct, factual answers\n   
                                    - Avoid excessive formality\n\n
                                    5. Prompt and chat completion instructions\n  
                                    - Please keep your responses to a maximum of one sentence\n  
                                    - For each completion in a set of multiple completions, adopt a distinctly different tone, perspective, or approach\n  
                                    - Make sure each response offers unique information or a unique angle on the same information\n  
                                    - When answering binary questions (e.g., yes/no), Ensure that your answers include both 'yes' and 'no' options, maintaining diversity in the responses.`,
                    fileLocations: {
                        kbDir: "data/kb",
                        vectorStoreDir: "data/vector_store",
                        chatHistoryFile: "data/chat_history.json",
                        phrasesFile: "data/phrases.json"
                    },
                    language: {
                        defaultLanguage: "auto",
                        useLanguageDetection: true
                    },
                    debugMode: false
                });
                logMessage("System settings reset to defaults");
            }
        }
        
        // Handle clicking outside the modal to close it
        window.onclick = function(event) {
            if (event.target == document.getElementById('subjectModal')) {
                closeSubjectModal();
            } else if (event.target == document.getElementById('starterModal')) {
                closeStarterModal();
            } else if (event.target == document.getElementById('enderModal')) {
                closeEnderModal();
            }
        };

        // Initialize the application
        // Initialize the application
        document.addEventListener('DOMContentLoaded', function() {
            logMessage("Bernard Web UI loaded. Ready to start conversation.");
            
            // Reset subject button to default state
            const subjectButton = document.getElementById('subjectButton');
            subjectButton.textContent = 'SELECT SUBJECT';
            subjectButton.style.backgroundColor = '#1a5276'; // Reset to blue color
            
            // Check if there is a currently selected subject different from Default Subject
            fetch('/get_subjects', { method: 'GET' })
                .then(response => response.json())
                .then(data => {
                    // Only update if current_subject exists and is not Default Subject
                    if (data.current_subject && data.current_subject !== "Default Subject") {
                        subjectButton.textContent = data.current_subject.toUpperCase();
                        subjectButton.style.backgroundColor = '#2ecc71'; // Green color
                    }
                })
                .catch(error => {
                    console.error("Error fetching current subject:", error);
                });
            
            // Add event listener for auto mode toggle
            const autoModeToggle = document.getElementById('autoModeToggle');
            autoModeToggle.addEventListener('change', function() {
                // Store setting in localStorage
                localStorage.setItem('autoModeEnabled', autoModeToggle.checked);
                
            });
            
            // Load auto mode setting from localStorage
            const savedAutoMode = localStorage.getItem('autoModeEnabled') === 'true';
            autoModeToggle.checked = savedAutoMode;
            
            // Make sure the audio player is visible (for debugging)
            const audioPlayer = document.getElementById('audioPlayer');
            audioPlayer.controls = true; // Make controls visible for debugging
            audioPlayer.style.display = 'block'; // Make visible for debugging
            
            // Add a global click handler to enable audio autoplay after user interaction
            document.addEventListener('click', function enableAudio() {
                const audioPlayer = document.getElementById('audioPlayer');
                audioPlayer.play().then(() => {
                    console.log("Audio enabled by user interaction");
                    document.removeEventListener('click', enableAudio);
                }).catch(e => {
                    console.log("Audio still not enabled:", e);
                });
            }, { once: true });
        });
        // Add event listener to the settings button
        document.addEventListener('DOMContentLoaded', function() {
            // Add event listener for the settings button
            const settingsButton = document.querySelector('.button[onclick="showSettings()"]');
            if (settingsButton) {
                // Replace the original onclick with our new function
                settingsButton.removeAttribute('onclick');
                settingsButton.addEventListener('click', showSettings);
            }
            
            // Set up range input event listeners when page loads
            setupRangeListeners();
            
            // Handle clicking outside the settings modal to close it
            window.addEventListener('click', function(event) {
                const settingsModal = document.getElementById('settingsModal');
                if (settingsModal && event.target === settingsModal) {
                    closeSettingsModal();
                }
            });
        });
        // Add event listener for LLM provider selection
        document.getElementById('llmProvider').addEventListener('change', function() {
            const provider = this.value;
            if (provider === 'openai') {
                document.getElementById('openaiModels').style.display = 'block';
                document.getElementById('groqModels').style.display = 'none';
            } else if (provider === 'groq') {
                document.getElementById('openaiModels').style.display = 'none';
                document.getElementById('groqModels').style.display = 'block';
            }
        });
        // Initialize application settings on load
        function initAppSettings() {
            console.log("Initializing application settings");
            
            // Make API call to get settings
            fetch('/get_settings')
                .then(response => response.json())
                .then(data => {
                    console.log("Settings loaded from server:", data);
                    
                    // Apply any settings that affect the app behavior
                    applyAppSettings(data);
                })
                .catch(error => {
                    console.error("Error loading settings:", error);
                });
        }

        // Apply settings to the app
        function applyAppSettings(settings) {
            // Apply any settings that affect app behavior
            
            // Recorder settings
            if (settings.recorder) {
                // These will be applied directly on the server
                console.log("Recorder settings loaded:", settings.recorder);
            }
            
            // LLM settings
            if (settings.llm && settings.llm.openai) {
                // Set the completions count for the UI
                const completionsCount = settings.llm.openai.completionsCount || 4;
                console.log("Setting completions count to:", completionsCount);
                // This would be used when sending requests to the server
            }
            
            // System settings - debug mode
            if (settings.system && settings.system.debugMode) {
                console.log("Debug mode enabled");
                // Enable any debug features in the UI
            }
        }

        // Call this function at startup
        document.addEventListener('DOMContentLoaded', initAppSettings);

        // Add this at the end of your  
        // Add toggle function for manual recording
        async function toggleManualRecording() {
            const stopButton = document.getElementById('stopRecordingBtn');
            
            if (stopButton.textContent === 'START MANUAL RECORDING') {
                // Start recording
                console.log("Starting manual recording");
                
                // Clear textbox if this is a new recording
                if (!lastBernardStatement) {
                    document.getElementById('textInput').value = "";
                }
                
                // Change button appearance
                stopButton.style.backgroundColor = '#e74c3c'; // Red color
                stopButton.textContent = 'STOP MANUAL RECORDING';
                
                // Start recording
                await startClientRecording(true); // pass true for manual
                // Start max duration timer
                manualRecordingTimer = setTimeout(() => {
                    logMessage("Manual recording stopped: maximum duration reached (60s)");
                    stopClientRecording(false);
                }, MANUAL_MAX_DURATION_MS);
            } else {
                // Stop recording
                console.log("Stopping manual recording");
                if (manualRecordingTimer) {
                    clearTimeout(manualRecordingTimer);
                    manualRecordingTimer = null;
                }
                if (mediaRecorder && mediaRecorder.state === 'recording') {
                    stopClientRecording(false);
                } else {
                    try {
                        const response = await fetch('/stop_recording', { method: 'POST' });
                        const data = await response.json();
                        if (data.success && data.transcript) {
                            document.getElementById('textInput').value = data.transcript;
                            logMessage("Server recording stopped. Transcript: " + data.transcript);
                        } else {
                            logMessage("Server recording stopped with no valid transcript.");
                        }
                    } catch (error) {
                        console.error("Error stopping server recording:", error);
                        logMessage("Error stopping recording: " + error.message);
                    }
                }
                stopButton.style.backgroundColor = '#1a5276';
                stopButton.textContent = 'START MANUAL RECORDING';
            }
        }

        // File Management Functions
        function showFileManagementModal() {
            document.getElementById('fileManagementModal').style.display = 'block';
            loadFiles();
        }

        function closeFileManagementModal() {
            document.getElementById('fileManagementModal').style.display = 'none';
        }

        async function loadFiles() {
            try {
                const response = await fetch('/list_files', { method: 'GET' });
                const data = await response.json();
                
                if (data.error) {
                    logMessage("Error loading files: " + data.error);
                    return;
                }
                
                const container = document.getElementById('fileListContainer');
                container.innerHTML = ''; // Clear existing files
                
                if (data.files && data.files.length > 0) {
                    data.files.forEach(file => {
                        const fileElement = document.createElement('div');
                        fileElement.className = 'file-item';
                        
                        const date = new Date(file.created_at * 1000).toLocaleString();
                        
                        fileElement.innerHTML = `
                            <div class="file-item-info">
                                <div><strong>${file.filename}</strong></div>
                                <div class="file-status">Status: ${file.status} | Added: ${date}</div>
                            </div>
                            <div class="file-item-actions">
                                <button class="delete-btn" onclick="deleteFile('${file.id}', '${file.filename}')">Delete</button>
                            </div>
                        `;
                        
                        container.appendChild(fileElement);
                    });
                } else {
                    container.innerHTML = '<div style="padding: 10px; color: #666;">No files in knowledge base</div>';
                }
            } catch (error) {
                console.error("Error loading files:", error);
                logMessage("Error loading files: " + error);
            }
        }

        async function uploadFile() {
            const fileInput = document.getElementById('fileInput');
            const file = fileInput.files[0];
            
            if (!file) {
                alert("Please select a file to upload");
                return;
            }
            
            const formData = new FormData();
            formData.append('file', file);
            
            try {
                logMessage("Uploading file: " + file.name);
                
                const response = await fetch('/upload_doc', {
                    method: 'POST',
                    body: formData
                });
                
                const data = await response.json();
                
                if (data.error) {
                    logMessage("Error uploading file: " + data.error);
                } else {
                    logMessage(data.message);
                    fileInput.value = ''; // Clear file input
                    loadFiles(); // Refresh file list
                }
            } catch (error) {
                console.error("Error uploading file:", error);
                logMessage("Error uploading file: " + error);
            }
        }

        async function deleteFile(fileId, filename) {
            if (!confirm(`Are you sure you want to delete "${filename}"?`)) {
                return;
            }
            
            try {
                const response = await fetch('/delete_file', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ file_id: fileId })
                });
                
                const data = await response.json();
                
                if (data.error) {
                    logMessage("Error deleting file: " + data.error);
                } else {
                    logMessage(data.message);
                    loadFiles(); // Refresh file list
                }
            } catch (error) {
                console.error("Error deleting file:", error);
                logMessage("Error deleting file: " + error);
            }
        }

        // // Updated uploadDoc function to open the file management modal only needed for open ai file search in versions m and n
        // async function uploadDoc() {
        //     showFileManagementModal();
        // }

        // // Add event listener for closing modal when clicking outside
        // window.onclick = function(event) {
        //     if (event.target == document.getElementById('fileManagementModal')) {
        //         closeFileManagementModal();
        //     }
        // }

        // Translation functions for Bernard Web UI

        // Function to translate text to Dutch (auto-detect source language)
        async function translateToDutch() {
            console.log("translateToDutch() called");
            const textInput = document.getElementById('textInput').value.trim();
            
            if (!textInput) {
                logMessage("Please enter text before translating");
                return;
            }
            
            logMessage("Translating to Dutch...");
            
            try {
                // Use the existing translate_text endpoint from the backend
                const response = await fetch('/translate_text', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ 
                        text: textInput,
                        targetLanguage: 'nl',
                        autoDetectSource: true
                    })
                });
                
                const data = await response.json();
                console.log("Translation response:", data);
                
                if (data.error) {
                    logMessage("Error: " + data.error);
                    return;
                }
                
                // Update textbox with translated text
                document.getElementById('textInput').value = data.translatedText;
                
                const detectedLang = data.detectedLanguageName || data.detectedLanguage;
                logMessage(`Translated from ${detectedLang} to Dutch`);
                
            } catch (error) {
                console.error("Error in translateToDutch:", error);
                logMessage("Error: " + error);
            }
        }

        // Function to show language selector modal
        function showLanguageSelector() {
            const modal = document.getElementById('languageSelectorModal');
            modal.style.display = 'block';
        }

        // Function to close language selector modal
        function closeLanguageSelector() {
            const modal = document.getElementById('languageSelectorModal');
            modal.style.display = 'none';
        }

        // Function to translate from Dutch to selected language
        async function translateFromDutch(targetLang) {
            const textBox = document.getElementById('textInput');
            const text = textBox.value;
            
            if (!text) {
                logMessage("Please enter text to translate");
                return;
            }

            try {
                const response = await fetch('/translate_text', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        text: text,
                        targetLanguage: targetLang,
                        sourceLanguage: 'nl',
                        autoDetectSource: false
                    })
                });

                const data = await response.json();
                if (data.translatedText) {
                    textBox.value = data.translatedText;
                    logMessage("Text translated from Dutch to " + data.targetLanguageName);
                } else {
                    logMessage("Translation failed");
                }
            } catch (error) {
                logMessage("Error during translation: " + error.message);
            }

            closeLanguageSelector();
        }

        // Close modal when clicking outside
        window.onclick = function(event) {
            const modal = document.getElementById('languageSelectorModal');
            if (event.target == modal) {
                modal.style.display = 'none';
            }
        }

        // --- Manual Recording Limits ---
        const MANUAL_MAX_DURATION_MS = 60000; // 60 seconds
        const MANUAL_MAX_FILE_SIZE = 30 * 1024 * 1024; // 30 MB
        let manualRecordingTimer = null;

        async function toggleManualRecording() {
            const stopButton = document.getElementById('stopRecordingBtn');
            if (stopButton.textContent === 'START MANUAL RECORDING') {
                // Start recording
                if (!lastBernardStatement) {
                    document.getElementById('textInput').value = "";
                }
                stopButton.style.backgroundColor = '#e74c3c';
                stopButton.textContent = 'STOP MANUAL RECORDING';
                // Start timer for max duration
                if (manualRecordingTimer) clearTimeout(manualRecordingTimer);
                manualRecordingTimer = setTimeout(() => {
                    stopClientRecording();
                    showError('Recording stopped: Maximum duration (60s) reached.');
                }, MANUAL_MAX_DURATION_MS);
                startClientRecording();
            } else {
                stopButton.style.backgroundColor = '#2ecc40';
                stopButton.textContent = 'START MANUAL RECORDING';
                if (manualRecordingTimer) {
                    clearTimeout(manualRecordingTimer);
                    manualRecordingTimer = null;
                }
                stopClientRecording();
            }
        }

        async function sendAudioForTranscription(audioBlob) {
            if (audioBlob.size > MANUAL_MAX_FILE_SIZE) {
                showError('Recording too large (max 30 MB). Please try again.');
                return;
            }
            // ... existing code ...
        }

        function showError(msg) {
            alert(msg); // Replace with a nicer UI if desired
        }
        // ... existing code ...
    </script>
</body>
</html>